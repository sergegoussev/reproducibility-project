[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "Code of Conduct\nWe are a community based on openness, as well as friendly and didactic discussions.\nWe aspire to treat everybody equally, and value their contributions.\nDecisions are made based on technical merit and consensus.\nCode is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions.\nWe abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-start.html",
    "href": "docs/reproducibility-guidance/how-to-start.html",
    "title": "How do I start?",
    "section": "",
    "text": "You’ve seen the ideal state, but how do you start?",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How do I start?"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-start.html#in-a-nutshell",
    "href": "docs/reproducibility-guidance/how-to-start.html#in-a-nutshell",
    "title": "How do I start?",
    "section": "In a nutshell",
    "text": "In a nutshell\nThe strategy to make a computational research project reproducible is to cast it into the framework of a reproducible analytical pipeline (RAP). This is a natural framework to represent a research project and, by following the framework, the research project inherits good properties for open and reproducible research. Structuring a research project as a RAP fits with the model of a research compendium, with the latter encompassing all parts of the project.\nStructuring a project as a RAP involves adopting some tools traditionally found in the realm of software development and using these to structure and automate many parts of a research project, enabling it to be reproducible. Although this has proven to be a popular and useful strategy in the world of open science, it comes with the disadvantage that it involves many new tools and ideas. Consequently, it can be difficult and time consuming to adopt the RAP framework.\nThe purpose of this guide is to give a gentler introduction that is useful for projects in the domain of price statistics.1 See the example for a pipeline that’s relevant for price statistics and incorporates most of these ideas.",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How do I start?"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-start.html#getting-started",
    "href": "docs/reproducibility-guidance/how-to-start.html#getting-started",
    "title": "How do I start?",
    "section": "Getting started",
    "text": "Getting started\nThe RAP maturity framework has three levels of maturity—baseline, silver, gold—that characterize the sophistication of a reproducible analytical pipeline. Incorporating all the features of a gold-level pipeline is a lot of work and not always appropriate for all projects. The guidance here is how to get started on making reproducible projects in the price-statistics domain that aligns with the RAP framework while focusing on the key pieces early.\nThe cornerstone of any reproducible project rests on five key idea.\n\nUse open tools for research so that anyone is free to use those same tools. In practice this tends to mean R and Python for empirical work.\nKeep track of the version of the project so that it is unambiguous how the research was done. In practice this means using git to manage the evolution of a project.\nExplain how to reproduce the project. In practice this means making a file called README.md to outlines the steps to reproduce the project (.md stands for markdown, an easy way to markup text).\nMake the project available for others. In practice this leverages 1, 2, and 3 by putting the steps to recreate a research project on a service like GitHub or GitLab.\nIf your reseach project trials a new method, if you can, evaluate it with publicly available data. There are a few open datasets that are commonly used in the discipline.\n\nAlthough the baseline level for a RAP involves more than just these things, these are the core features of any reproducible project.\nThe biggest hurdle to making a project reproducible is using git. This is a complex piece of software intended for software developers and can feel frustrating and unnecessary if you’re not used to it. (But trust me: once you get it, you’ll never want to turn back.) The Turing Way has a nice introduction to version control and git for researchers. Happy Git and GitHub for the useR is a more involved introduction to git and github. Although it is targeted primary at users of R, most of the ideas are not restricted to R.",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How do I start?"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-start.html#levelling-up",
    "href": "docs/reproducibility-guidance/how-to-start.html#levelling-up",
    "title": "How do I start?",
    "section": "Levelling up",
    "text": "Levelling up\nThe next step towards making a project reproducible involves putting some structure on the project and following certain conventions. This makes it easier for someone to replicate your research, but it also much easier to execute your project because it follows a proven recipe—no need to reinvent it.\nThe key improvements involve structuring how your project is organized.\n\nFollow a standard directory structure.\nManage the computational environment.\nAutomate the execution of the project.\n\nIn each case there are several ways to accomplish these things. For example, R and Python have different (but not disjoint) tools for managing packages with an eye towards reproducibility, each with their own tradeoffs and resulting degree of reproducibility. However, while the details may differ depending on the specific tools, the overall idea is the same.",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How do I start?"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-start.html#mastering-reproducibility",
    "href": "docs/reproducibility-guidance/how-to-start.html#mastering-reproducibility",
    "title": "How do I start?",
    "section": "Mastering reproducibility",
    "text": "Mastering reproducibility\nThe final steps to make a research project highly reproducible are less about structure and more about how the computational parts of the research are done. Much like having clean and well-structured proofs is important for theory work, the scripts and code for an empirical project should not just be executable but also understandable.\n\nFollow a style guide.\nStructure and document code in a way that makes it easy to follow.\nHave automated tests.\n\nAlthough this may feel like a foray away from research and into software development, once you adopt these workflows it is hard to go back. Building reproducible analytical pipelines with R gives a book-length treatment of RAPs with a nice focus on these elements of reproducibility.",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How do I start?"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-start.html#checklist",
    "href": "docs/reproducibility-guidance/how-to-start.html#checklist",
    "title": "How do I start?",
    "section": "Checklist",
    "text": "Checklist\nRather than a strictly-defined list of requirements, this list is intended to provide a checklist of suggestions for how you can improve your project with RAP principles. You’re encouraged to consider the objective of your work and then determine which suggestions best support those objectives. Remember that any effort to make a paper replicable, however small, can turn an interesting paper and a vital reference tool for future researchers.\n\nBaseline\n\nMake the code available online. GitHub is free and considered the best option, but using an institution’s website, or even a public blog, is better than nothing.\nUse a widely-accepted language, ideally an open-source one. R and Python are both free and widely used, and using a commonly-understood language makes the code more accessible.\nMake input data available online when possible. Zenodo is a free-to-use option. You can also use existing open datasets if you can’t or don’t wish to go through the process of making your data availible.\nMake documentation explaining the code and data available online. These should be sufficient for a user to repeat the analysis.\n\n\n\nSilver\n\nThe code repository is well-structured, conforming to a well-known standard (such as standard directory format) or other logical layout.\nThe code adheres to a coding standard for the language, such as PEP8 for Python.\nThe data are well-structured, conforming to a well-known standard (such as Tidy data format) or other logical layout and include suitable metadata.\nThe explanatory documentation includes docstrings, methodology explainers, dependency/computational environment information, and more comprehensive guidance for less confident users.\n\n\n\nGold\n\nThe code repository employs tools to provide additional utility (such as CI/CD or GitHub Actions).\nThe code includes functions to help the user run it, or is fully packaged.\nThe output data’s metadata is comprehensive (e.g. frequency of update, quality controls, license requirements).\nThe explanatory documentation covers major changes over time (e.g., Semantic Versioning).\nThe project is automated end-to-end, including the writeup of the paper published alongside the code. In essense, it is clear how each figure and table in the paper was generated.",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How do I start?"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-start.html#footnotes",
    "href": "docs/reproducibility-guidance/how-to-start.html#footnotes",
    "title": "How do I start?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe RAP framework is not limited to research projects and is also useful for the regular production of price statistics.↩︎",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How do I start?"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/opening-data.html",
    "href": "docs/reproducibility-guidance/howtos/opening-data.html",
    "title": "What to do with data your research project creates",
    "section": "",
    "text": "CautionWIP\n\n\n\nThis page is still in the works. Possible topics:\n\nWhen to push your /output/ data to zenodo so that others can benefit from it or to keep it in your GitHub repo.\nNote, the guide should not focus on opening a new dataset that can be used as input for the resarch. This is covered in the contributing to the catalog section.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The data",
      "What to do with data your research project creates"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/input-data.html",
    "href": "docs/reproducibility-guidance/howtos/input-data.html",
    "title": "What to do with input data to your project",
    "section": "",
    "text": "CautionWIP\n\n\n\nThis page is still in the works. Overview of possible topics to summarize:\n\nSave input data into /data/ folder and use .gitignore to ensure that the raw data is not saved - as per the deeper dive on structure:\n\nIf data and analysis is simple, the analysis scripts in /src/analysis/ will take the data and generate relevant outputs (data and visuals) in /output/\nIf data cleaning is more complex, you can create a /data/raw, a /data/clean/, and a /src/data_cleaning.py that converts from raw to clean (before analysis). This way anyone can reproduce this process and modify the analysis with new data as they can understand exactly how to preprocess the data before analysis.\n\nUse precommit hooks to ensure that analysis notebooks don’t render the output that may be sensitive. precommit hooks to ensure privacy\nMake sure you don’t commit other sensitive information with the code and writeup - like access tokens or secrets. There are ways to set this up in a way that others can repeat that doesn’t commit it in git.\nOff course avoid mentioning sensitive things in the prose (say the documentation)\n\nNOTE: Use of these best practices is key you use sensitive or confidential data. For public data, .gitignore is still a good practice so that you don’t repost the raw data. Should also touch upon how researchers should approach propietary datasets (#46)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The data",
      "What to do with input data to your project"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/metadata.html",
    "href": "docs/reproducibility-guidance/howtos/metadata.html",
    "title": "How metadata helps the research process",
    "section": "",
    "text": "CautionWIP\n\n\n\nThis page is still in the works. The guide (#51) could cover such topics as:\n\nMetadata is to help find, reuse, understand everything. How-to-fair guide on the topic provides an intro, the FAIR cookbookis also not bad.\nPossible metadata that are key to know about:\n\nMetadata that helps findability - ideally all objects of a research process should have persistent identifiers (or PIDs) so that these can be easily found and be citable: In price statistics, some already exist/are possible, and others are not yet set up:\n\nResearchers in the discipline can sign up to create an ORCID. This helps you be found and get fair recognition for your work.\nDatasets published to data repositories like zenodo help mint DOIs. TBC how to handle proprietary datasets though (i.e. #46)\nPapers in official journals have DOIs that the journal creates as part of the publication process. Ideally papers published as part of conference proceedings could also have DOIs (as many disciplines now do), however this isn’t yet done in price stats.\nCode (i.e. the research compendium) is published in a way that mints a persistent identifier. Note that GitHub doesn’t mint a DOI but that may be okay for interim code and published code could be pushed to zenodo (which does).\n\nMetadata that helps interoperability:\n\nThe descriptive and structural metadata (i.e. info about each dataset) is outlined in the catalogue – hence we aim to help solve some of this with the catalogue.\n\nWhile not exclusive, we are trying to follow the basic dublin core\nThe way we define various things is as standard as possible so that its easy to use\n\nThe idea is that researchers (and their programs) can more easily understand open datasets they use for their research, understand them, etc.\n\nAccessiblity:\n\nWays to get the data is as simple as possible - say using download_zenodo (in R) to automate the downloading of data via its DOI\n\nMetadata that helps reusability:\n\nKnowing how datasets or code is licensed so that you know when and how to research it. We document this in the limitations section of the dataset record\nProvenance is clear. Say a dataset is made available on zenodo - where it came from and how it was created/modified is clear.\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How metadata helps the research process"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/citing-code.html",
    "href": "docs/reproducibility-guidance/howtos/citing-code.html",
    "title": "How to cite code",
    "section": "",
    "text": "Citing software is historically not something that researchers have done. Code can often be hard to cite and the software used to derive the results of a paper was usually seen as a “detail” that is not worth mentioning in the body of paper. (Perhaps to avoid too many questions about how, exactly, the results in the paper were derived.) The last decade has seen a move towards more reproducible research in economics—mainstream journals require code and usually data for quantitative papers—and the proliferation of open-source research software has made it easier to reliably cite the software used for research.\nCiting code is particularly important in the area of price statistics to facilitate reproducible research. There are a great many price index methods, often with fiddly variations, used by different researchers and it is important to know the exact implementation used to generate a price index in order to reproduce the construction of that index. Research for price statistics is also usually done by government agencies; transparency about the code to generate the results that inform the methods used by government agencies is important to maintain the trust in official statistics.\nFor further reading, the Turing Way provides a good overview for citing code in research and represents the current state of the world of open-source research software.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to cite code"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/citing-code.html#citing-research-software",
    "href": "docs/reproducibility-guidance/howtos/citing-code.html#citing-research-software",
    "title": "How to cite code",
    "section": "Citing research software",
    "text": "Citing research software\nUnlike academic papers, there is no one way to cite a piece of software. How to cite a piece of software usually depends on how easy the author makes it. For example, R packages are easy to cite within R using the citation() function.\n\ncitation(\"IndexNumR\")\n\nTo cite package 'IndexNumR' in publications use:\n\n  White G (2023). _IndexNumR: Index Number Calculation_. R package\n  version 0.6.0, &lt;https://github.com/grahamjwhite/IndexNumR&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {IndexNumR: Index Number Calculation},\n    author = {Graham White},\n    year = {2023},\n    note = {R package version 0.6.0},\n    url = {https://github.com/grahamjwhite/IndexNumR},\n  }\n\nATTENTION: This citation information has been auto-generated from the\npackage DESCRIPTION file and may need manual editing, see\n'help(\"citation\")'.\n\n\nModern R packages tend to have their own website with this information on display and all CRAN packages get a DOI to facilitate referencing the use of R packages. Things are less standardized in the Python ecosystem, but the same ideas apply to make projects citeable (e.g., pandas).\nAnother way to cite a piece of research software is to the cite the paper introducing this software, usually in a journal like the Journal of Open Source Software, the Journal of Statistical Software, or the R Journal.\n@article{RJ-2021-038,\n  author = {Saavedra-Nieves, Alejandro and Saavedra-Nieves, Paula},\n  title = {IndexNumber: An R Package for Measuring the Evolution of Magnitudes},\n  journal = {The R Journal},\n  year = {2021},\n  note = {https://rjournal.github.io/},\n  volume = {13},\n  issue = {1},\n  issn = {2073-4859},\n  pages = {253-275}\n}\nFinally, for code that is not in a mainstream repository like CRAN, or does not have a website with citation information, citation information can sometimes be found in the source code. Github helps to find the citation information for a package and displays a button to cite the repository.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to cite code"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/citing-code.html#making-software-citable",
    "href": "docs/reproducibility-guidance/howtos/citing-code.html#making-software-citable",
    "title": "How to cite code",
    "section": "Making software citable",
    "text": "Making software citable\nGiven the variety of ways to cite research software, the key to making it citeable is making it easy to generate a reference for that software. For R packages this happens automatically if the package is available on CRAN; for Python packages (and R packages not on CRAN), a service like Zenodo can be used to get a DOI to facilitate referencing the software. Although consumers of software tend not to get it directly from a source-code repository, citation metadata can be added to github repositories to make them more citeable.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to cite code"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/citing-code.html#example",
    "href": "docs/reproducibility-guidance/howtos/citing-code.html#example",
    "title": "How to cite code",
    "section": "Example",
    "text": "Example\nThe {piar} R package is an example of a piece of software for making price indexes that is highly citeable.\n\ncitation(\"piar\")\n\nTo cite package 'piar' in publications use:\n\n  Martin S (2024). \"piar: Price Index Aggregation R.\" _Journal of Open\n  Source Software_, *9*(101), 6781. doi:10.21105/joss.06781\n  &lt;https://doi.org/10.21105/joss.06781&gt;.\n\n  Martin S (2025). _piar: Price Index Aggregation_.\n  doi:10.5281/zenodo.10110046\n  &lt;https://doi.org/10.5281/zenodo.10110046&gt;, R package version 0.8.2,\n  &lt;https://cran.r-project.org/package=piar&gt;.\n\nTo see these entries in BibTeX format, use 'print(&lt;citation&gt;,\nbibtex=TRUE)', 'toBibtex(.)', or set\n'options(citation.bibtex.max=999)'.\n\n\nThis information is displayed on the project website and CRAN. The citation information is contained in the source code, and consequently displayed by github, and the readme for the project is adorned with badges giving citation information. The goal is to have citation information available at each entry point at which a prospective user may first engage with {piar} and to have it be easy to add to a reference list.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to cite code"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/research-compendium-structure.html",
    "href": "docs/reproducibility-guidance/howtos/research-compendium-structure.html",
    "title": "Research compendium: deeper dive",
    "section": "",
    "text": "The purpose of this guide is on clarifying why you should care about structure and automation when it comes to initial setup and use of your research compendium as you are expanding it during your research. While these are slightly more advanced topics (we see structure as silver level and automation as gold), we think that considering the design at the beginning and a bit of discipline throughout adds great value to yourself and to others! You will indeed thank yourself later!",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "Research compendium: deeper dive"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#overview-of-the-structure",
    "href": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#overview-of-the-structure",
    "title": "Research compendium: deeper dive",
    "section": "Overview of the structure",
    "text": "Overview of the structure\nIn a nutshell, along with a resarch paper, a compendium is shared that includes all research objects necessary to reproduce the research. These are often git repositories (in say GitHub) that include a structure similar to the one in Figure 1 below.\n\n\n\n\n\n\nFigure 1: Exemplar price index pipeline.\n\n\n\n\nA little about each aspect\nA data folder that outlines where to store the raw dataset used for the research project. Ideally the researcher uses an open dataset (which will make the whole process reproducible), but they may also use a proprietary dataset.\n\n\n\n\n\n\nTipDon’t version control the main input dataset\n\n\n\nThe dataset that acts as the main input dataset to the research should not be version controlled. The folder is created in order to ensure that when a local copy of the compendium is used by researchers, they know where to put the data to ensure that the code will replicate the results in full.\nTechnically, this means making sure that the .gitignore skips this data file\n\n\nA folder for functions (or other code) that helps process your data into the relevant outputs. This could include the code to clean and prepare the raw dataset for research purposes, the code to create the processing and research experiments, as well as notebooks where the data is explored and various aspects that feed the research paper are generated.\nA folder for the output data. This data can be versioned (if it is not sensitive) with the repository and allows researchers to replicate the process. Note, if the output data can be used for research in its own right, it may be appropriate to register this dataset on a public repository (such as Zenodo) that mints a DOI.\nA folder for documentation to that explain key aspects of the research. This folder stores project documentation or the project design, however code should also be documented well.\nA license. This will tell users how they can use the code.\nA .gitignore file. There are some files and folders that should not be version controlled. Notable example is datasets\nA file to recreate the environment on which the code will run identically. A shift in one package version to another may change the output, hence its key to track exactly how to replicate the same environment and get the same result.\nFinally, a README to introduce the project. This will be the landing place when someone navigates to the repository, hence it should describe the project at a glance.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "Research compendium: deeper dive"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#how-to-share-a-research-compendium",
    "href": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#how-to-share-a-research-compendium",
    "title": "Research compendium: deeper dive",
    "section": "How to share a research compendium?",
    "text": "How to share a research compendium?\nVersion controlling the research compendium is key.1 Once version controlled, it is best to push it to a hosting service like GitHub. Working with something like GitHub makes it easy for researchers to ensure their projects are well structured and robust.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "Research compendium: deeper dive"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#why-use-a-standard-directory-structure",
    "href": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#why-use-a-standard-directory-structure",
    "title": "Research compendium: deeper dive",
    "section": "Why use a standard directory structure?",
    "text": "Why use a standard directory structure?\nAs outlined in the ideal case, a structured approach is at a minimum organized and clear. Anyone can:\n\nPut the same input data as you used in the /data/ folder.\nRun the preprocessing scripts in /src/processing.\nRepeat the analysis notebooks in /src/analysis/and get the same results.\n\nIn other words, the research becomes very clear and the value added is the contribution, with a lower cognitive load placed on anyone (including yourself at a later date) trying to understand it. The project probably had a messy but implicit structure, you are just making it explicit and coherent.\nWhat happens if you don’t make the structure explicit but keep it implicit?\n\nWhen you come to it fresh (either it is new for you or you are coming back to your own material much later), you waste time trying to get into the context.\nThe implicit structure (by nature of being implicit) is likely to be messier than if it was well thought through, hence it’s more likely to be harder to extend the research,\nThe cognitive burden placed on the prerequisite of understanding something not clearly structured means it’s less likely to be material you learn from. In other words, fewer people will review, evaluate or try what you are doing—meaning that your contribution stays in the generalizable category.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "Research compendium: deeper dive"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#why-take-the-effort-of-avoiding-manual-steps",
    "href": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#why-take-the-effort-of-avoiding-manual-steps",
    "title": "Research compendium: deeper dive",
    "section": "Why take the effort of avoiding manual steps?",
    "text": "Why take the effort of avoiding manual steps?\nFurther to structuring your project, avoiding manual steps (especially manipulation of spreadsheets!!) is key. If you’ve ever read a paper where preprocessing was described in a few short and unclear sentences which made it nearly impossible to reproduce the process without contacting the researcher—you can probably see what we mean!\nSo what to do instead?\n\nEnsure all steps can be executed through code (and version control this code with the repository). In other words, make sure there are no manual steps. This also means that you should version control all code, including the data preprocessing code.\nStructure the steps to follow a clear order of operations and use a specific folder structure for code and data that is processed along the way.\nTest the automation to ensure that anyone (including yourself at a later time) can process the same outputs with the same raw data.\nIf you’re up for it, you could even automate the rendering of the analysis paper, such as through the manuscripts feature of Quarto.\n\nFor instance Figure 2 shows visually how a .qmd file and some notebooks in the repository can be automated to generate the final analysis paper.\nUsing make or dvc are other examples of tools to automate the process.\n\n\n\n\n\n\n\n\nFigure 2: See walk-through by Alex Emmons for more detail.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "Research compendium: deeper dive"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#why-invest-in-documentation",
    "href": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#why-invest-in-documentation",
    "title": "Research compendium: deeper dive",
    "section": "Why invest in documentation?",
    "text": "Why invest in documentation?\nNo matter how clear the structure or the script naming convention, the narrative explanation of steps to follow will still requires some documentation. Indeed the list of specific steps may be clear from the analysis steps, but the why and the how will not be—hence both will also benefit from being made explicit.\nA good way to provide clarity to any user without the context is to create a visual of the process flow. For instance, you can outline the process using mermaid diagrams, or use open source diagram tool (like draw.io or lucidchart) to draw the process yourself, export the image and embed that into the documentation (be sure you version to diagram file itself so that you can always tune the diagram and don’t have to recreate it!).2\n\n\n\n\n\n\nNoteWe use draw.io for reproducibility diagrams\n\n\n\nAll raw draw.io and exports for this site are saved in /docs/images/",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "Research compendium: deeper dive"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#notable-example",
    "href": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#notable-example",
    "title": "Research compendium: deeper dive",
    "section": "Notable example",
    "text": "Notable example\nTo showcase an exemplar for price statistics, we created a mock-up price index pipeline that researchers can explore.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "Research compendium: deeper dive"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#footnotes",
    "href": "docs/reproducibility-guidance/howtos/research-compendium-structure.html#footnotes",
    "title": "Research compendium: deeper dive",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee overview and explanations of version control in The Turing Way for more info.↩︎\nCheck out this overview guide on process mapping by the NHS.↩︎",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "Research compendium: deeper dive"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html",
    "href": "docs/reproducibility-guidance/intro.html",
    "title": "How to approach reproducibility",
    "section": "",
    "text": "For robust scientific progress, new methods or approaches should be confirmed independently before they are widely adopted. The goal of appropriately structuring and sharing research objects in a transparent fashion is to simplify this peer review process.1 The primary way this is achieved by creating and publicly publishing a research compendium along with the paper. A research compendium is a collection of digital parts of the research project that supports reuse, including data, code, protocols, documentation, metadata, etc.2",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How to approach reproducibility"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#what-is-reproducibility",
    "href": "docs/reproducibility-guidance/intro.html#what-is-reproducibility",
    "title": "How to approach reproducibility",
    "section": "",
    "text": "For robust scientific progress, new methods or approaches should be confirmed independently before they are widely adopted. The goal of appropriately structuring and sharing research objects in a transparent fashion is to simplify this peer review process.1 The primary way this is achieved by creating and publicly publishing a research compendium along with the paper. A research compendium is a collection of digital parts of the research project that supports reuse, including data, code, protocols, documentation, metadata, etc.2",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How to approach reproducibility"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#why-does-it-help",
    "href": "docs/reproducibility-guidance/intro.html#why-does-it-help",
    "title": "How to approach reproducibility",
    "section": "Why does it help?",
    "text": "Why does it help?\nThe main principle of the research compendium is to provide all the information about the project publicly and structured in a clear and logical way to ensure that its use is straightforward. This helps researchers themselves as they can easily jump back to a previous project, simplify the task for reviewers or those who want to extend the research, as well as those simply looking to learn. If done properly, the research compendium will help:3\n\nImprove the transparency, reliability and reproducibility of research.\nSimplify peer review.\nFacilitate data and code sharing.\nAllow easy extension of the research.\nEnable learners to understand the research.\nMake it much easier to transition a new method to production.4\nIncrease research visibility and citations.",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How to approach reproducibility"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#key-components-in-a-research-project",
    "href": "docs/reproducibility-guidance/intro.html#key-components-in-a-research-project",
    "title": "How to approach reproducibility",
    "section": "Key components in a research project",
    "text": "Key components in a research project\nTo make a research project reproducible, it is key to to observe that there is not one object that is produced (i.e. the paper), but 4. Figure 1 provides a visual that can help understand what each is intended to for and how to approach each for reproducibility.\n\n\n\n\n\n\nFigure 1: Research process\n\n\n\n\n(1) The data\nThe input data is one of the first and most critical aspects of empirical research. Whatever research questions are asked, they need to be evaluated using one or several datasets.\n\nIdeally, input datasets are openly available, as this ensures that the dataset can be used as a benchmark for many different researches over time and help ensure that true reproduciblity is possible. To help increase the use of open datasets, this project includes a catalogue that lists key datasets in the discipline and documents each to help understand it with minimal effort.\nThe input data is owned (and hosted) separately from the research, hence it is not republished with the research project. Ideally the research project automates the collection and processing of the data\n\nIt is key to differentiate input data and output data:\n\nInput data is the data used to answer research questions. It may be open, closed (proprietary or sensitive), or synthetic. Each is approached separately.\nOuput data is data created as part of the research process. Whether (and how) its shared depends on various circumstances and value to later research projects (to say act as an input dataset for other projects).\n\n\n\n(2) The research compendium\nThe research compendium is a structured object that contains all the aspects to recreate the output (i.e. results shared in the paper). Some key sub-components include:\n\nIts logical structure ensures maturity and helps others (or you later) in trying to peer review, reproduce or extend the research.\nThe code and workflows ingest and processes input data to create relevant outputs (output data, figures, etc) in an automated fashion (i.e. avoiding manual steps). As mentioned above, the input data itself however is not a part of the compendium and is not republished.\nThe source for the paper is included so that raw input to final paper can be reproduced in an automated fashion. This allows you to regenerate and change the paper as needed.\n\nIt is key to differentiate between active and archived versions of the compendium:\n\n2A. Active research compendium\nWhen a project is active, it is best to use a version control process (such as GitHub/GitLab) to store the whole object. While GitHub/GitLab are great for technical maturity and robustness, they are not permanent records - i.e. the GitHub repository can be deleted or changed, making it hard to find later.\n\n\n2B. Archived research compendium\nWhen a research project is complete, the compendium should thus be archived in a repository. The snapshot created at this time should align to the paper that was published. Repositories (such as Zenodo) mint DOIs (or Digitial Object Identifiers) that are unique and are immutable (i.e. don’t change). This ensures that you can find that same snapshot at any point in the future.\n\n\n\n\n\n\nNoteAdoption depends on the organization\n\n\n\nUse of repositories such as Zenodo depends on the researchers and the organization they are in.\n\n\n\n\n\n(3) Key software\nNot all code in the research compendium will be written by the researchers from scratch using vanilla code. It is likely to use packages to abstract away some or most of the complexity and automate the work. There are two sensible rules of thumb that can be applied:\n\nIf the code is pivotal to the methodology (for instance the package was used to calculate price indices), then it is best to acknowledge this in the paper (by citing it).\nA snapshot of the computational environment should be taken and included in the research compendium. This ensures that the exact computation requirement can be replicated, but also to ensure that dependencies on specific versions of other packages is saved.\n\n\n\n(4) The paper\nFinally, the paper is generated as the output. As the research compendium should (ideally) have the source to generate the paper (such as by using Quarto to render a paper.qmd as a pdf), outputs or images in the paper can be traced back from the body of the image to the code that generated them.",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How to approach reproducibility"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#footnotes",
    "href": "docs/reproducibility-guidance/intro.html#footnotes",
    "title": "How to approach reproducibility",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA good overview of reproducible research is done by the Turing Way. The book also covers open research and many other topics.↩︎\nSee Research Compendia in The Turing Way for more detail on this concept.↩︎\nSee a more in depth overview of the benefits of reproducible research, as well as common barriers.↩︎\nResearch compendia are conceptually quite similar to Reproducible Analytical Pipelines (RAPs), although the latter focuses more on production processes. Hence if the research is easy to reproduce by adopting a compendium structure and making everything easily reproducible, operationalizing of a new method could be dramatically simplified. For more on RAPs in price statistics, see a RAP class recently done by ESCAP, as well as a good paper by Price and Marques (2023) showcasing RAP for production processes.↩︎",
    "crumbs": [
      "How to be reproducible",
      "Getting started",
      "How to approach reproducibility"
    ]
  },
  {
    "objectID": "docs/catalogue/about.html",
    "href": "docs/catalogue/about.html",
    "title": "How does the catalogue work?",
    "section": "",
    "text": "Researchers often struggle to use open data. They may find it hard to find open datasets for their research projects. If they find open datasets, they may struggle to easily understand them in order to confirm which works best for their research. Finally they may find it challenging to know how to work with or even how to cite the dataset.\nThe Price Statistics Open Data Catalogue helps find, assess, and understand how to access open datasets.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How does the catalogue work?"
    ]
  },
  {
    "objectID": "docs/catalogue/about.html#how-the-price-statistics-open-data-catalogue-works-in-a-nutshell",
    "href": "docs/catalogue/about.html#how-the-price-statistics-open-data-catalogue-works-in-a-nutshell",
    "title": "How does the catalogue work?",
    "section": "How the Price Statistics Open Data Catalogue works in a nutshell",
    "text": "How the Price Statistics Open Data Catalogue works in a nutshell\nThe idea is simple. The catalogue lists open datasets relevant to the discipline and is searchable according to standard tags (more on tags below). It also provides basic information about each dataset that allows researchers to assess its relevance and know how to access it. Visually this is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Basic idea of how we see the data catalogue working.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How does the catalogue work?"
    ]
  },
  {
    "objectID": "docs/catalogue/about.html#what-are-the-tags-used-in-the-catalogue",
    "href": "docs/catalogue/about.html#what-are-the-tags-used-in-the-catalogue",
    "title": "How does the catalogue work?",
    "section": "What are the tags used in the catalogue?",
    "text": "What are the tags used in the catalogue?\nThe catalogue uses the following tag structure to help categorize the datasets.\n\n\n\n\n\n\nNoteTags may change\n\n\n\nTags will evolve and change over time, especially as new datasets are registered.\n\n\n\nData type\nThe first set of tags categorize the dataset on common types used in price statistics.\n\nscanner\nweb-scraped\nadministrative\nfield-or-sample\n\n\n\nDataset Topic\nThe second set of tags focus on the uses of the data within price statistics. This relates most closely to categories of the area being measured as this implies the use of specific features in the data and specific set of methods.\n\nelectronics-and-applicances\nhousing\ngroceries-and-food\nfuels\n\n\n\n\n\n\n\nNoteLabels to be expanded over time\n\n\n\nAs new datasets are catalogued, this list will be expanded.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How does the catalogue work?"
    ]
  },
  {
    "objectID": "docs/catalogue/about.html#where-is-the-standalone-data-catalogue",
    "href": "docs/catalogue/about.html#where-is-the-standalone-data-catalogue",
    "title": "How does the catalogue work?",
    "section": "Where is the standalone data catalogue?",
    "text": "Where is the standalone data catalogue?\nThe standalone data catalogue can be found here. It is a static site hosted on a separate GitHub repository.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How does the catalogue work?"
    ]
  },
  {
    "objectID": "docs/catalogue/about.html#what-does-the-catalogue-not-do",
    "href": "docs/catalogue/about.html#what-does-the-catalogue-not-do",
    "title": "How does the catalogue work?",
    "section": "What does the catalogue not do?",
    "text": "What does the catalogue not do?\nAs the catalogue does not store the dataset itself or make decisions on key aspects of the dataset, but simply describes it in detail. In other words, this catalogue is not a data repository. Catalogue records point to the wherever the dataset lives. If more than one version is available, the dataset version that is easiest for researchers to use is referenced.\n\n\n\n\n\n\nNoteThis is an interim catalogue only!\n\n\n\nThis will likely not be the long-term stable data catalogue used in the discipline. The idea, however, it to start with this interim (and very simple open-source) catalogue, while the project investigates a more viable longer term solution.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How does the catalogue work?"
    ]
  },
  {
    "objectID": "docs/catalogue/contributing.html",
    "href": "docs/catalogue/contributing.html",
    "title": "How can you contribute?",
    "section": "",
    "text": "If you have a dataset that you would like to register on the catalogue, the following process outlines how to do this. Figure 1 outlines this in high level, with details on each step below.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How can you contribute?"
    ]
  },
  {
    "objectID": "docs/catalogue/contributing.html#requirements-to-contribute-to-the-data-catalogue",
    "href": "docs/catalogue/contributing.html#requirements-to-contribute-to-the-data-catalogue",
    "title": "How can you contribute?",
    "section": "Requirements to contribute to the data catalogue",
    "text": "Requirements to contribute to the data catalogue\nIn order to contribute to the catalogue, the following criteria must be met:\n\nThe dataset should be publicly available for researchers. There are proprietary datasets that could in theory also be listed, however until the price statistics reproducibility project figures out the process for this, we request that only fully open datasets are registered. We encourage requests on valuable proprietary datasets, however these will not be catalogued until the process is flushed out.\nThe dataset must be related to the price statistics discipline. Price statisticians most typically track change in prices, such as through price index methods – thus the dataset should support this use case. Other use cases, such as for machine learning applications when it comes to classification, can also be submitted, but should be as close to the needs of the discipline as possible.\nBe of value to the discipline. Many data catalogues that are too lax with the cataloguing process become filled with many datasets of incremental value. As a result, users struggle to find highly valuable datasets, which eventually causes a dropoff in use of the catalogue. To avoid this, the value of the dataset to researchers in the discipline should be clear. The reproducibility team will review and approve each proposed submission during each meeting.\nThe contributor must document the dataset in full when the dataset is to be registered. Having partially documented datasets on the catalogue will take away from user experience and will thus takeaway from the push to be open. A Maturity model of registered datasets is provided below to showcase how to document a dataset.\nThe dataset should be real, although artificial and modified datasets are accepted if they are of value to reproducibility. Specifically, synthetic datasets generated as part of a research process may not need to be registered if they can be reproduced through code published with the research, in which case we recommend that the code that generated it be made publicly available as part of that research projects’ research compendium.\n\nSome synthetic/artificial datasets may be proposed, such as if the artificial dataset has become of high value and is used everywhere as if its a real dataset (such as the Turvey dataset).\n\nThe dataset is already published somewhere easy to download and in a machine readable format. Make sure that users can download and use the dataset easily. Several options for hosting a dataset are possible (see next section on where to host datasets).",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How can you contribute?"
    ]
  },
  {
    "objectID": "docs/catalogue/contributing.html#where-to-host-the-dataset-itself",
    "href": "docs/catalogue/contributing.html#where-to-host-the-dataset-itself",
    "title": "How can you contribute?",
    "section": "Where to host the dataset itself",
    "text": "Where to host the dataset itself\nAs the price statistics data catalogue describes datasets already hosted elsewhere (in other words it is not a data repository), the first step is to host the dataset somewhere. This is fundamentally up to the researcher and the institution they work at. Ideally a data repository is used that mints a Digitial Object Identifier (DOI) so that the dataset can be easily cited and found. Data repositories often allow a researcher to host private datasets and lock down access but still create a DOI. Regardless of where, the DOI should be ready when the dataset is registered in the catalogue so that researchers can find the dataset itself and know how to cite the dataset.\nRead more about data repositories in the Turing Way.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How can you contribute?"
    ]
  },
  {
    "objectID": "docs/catalogue/contributing.html#level-1",
    "href": "docs/catalogue/contributing.html#level-1",
    "title": "How can you contribute?",
    "section": "Level 1",
    "text": "Level 1\nThis level sets a bare minimum a dataset needs to have to be registered to the catalogue\n\nThe dataset has a basic description to introduce it to any users in the discipline\nThe data model (structure of the data and each variable) is documented\nThe dataset is available openly and the data file format is anything that is machine readable (for instance a proprietary format like .xlsx or language specific data formats like .Rdata are fine, however a pdf is not) is referenced.\nThe license (or at minimum terms of the use of the dataset) is listed so that it is clear how the dataset can be used and how it cannot.\nInformation on how to cite the dataset is available.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How can you contribute?"
    ]
  },
  {
    "objectID": "docs/catalogue/contributing.html#level-2",
    "href": "docs/catalogue/contributing.html#level-2",
    "title": "How can you contribute?",
    "section": "Level 2",
    "text": "Level 2\nLevel 2 implies a higher level of maturity to simplify the process for data\n\nThe dataset is stored in an open file format\nDataset quality considerations and detailed nuances of the data are discussed. This is best done in specific quality section of each variable or for the table/dataset as a whole.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How can you contribute?"
    ]
  },
  {
    "objectID": "docs/catalogue/contributing.html#level-3",
    "href": "docs/catalogue/contributing.html#level-3",
    "title": "How can you contribute?",
    "section": "Level 3",
    "text": "Level 3\nThis implies a ‘gold standard’ for a dataset\n\nThe dataset is made available in a data repository (such as Zenodo) that mints a DOI. This DOI is listed as part of the dataset.\nA data paper detailing the dataset is available and linked. Alternatively, a blog can also be written on the project site to introduce the dataset with the dataset owner.",
    "crumbs": [
      "Data Catalogue",
      "About the catalogue",
      "How can you contribute?"
    ]
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "",
    "text": "Welcome to the project aiming to help researchers in the field of price statistics make their projects more reproducible and help push open science in the discipline."
  },
  {
    "objectID": "docs/index.html#making-guidance-on-reproducibility-available",
    "href": "docs/index.html#making-guidance-on-reproducibility-available",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Making guidance on reproducibility available",
    "text": "Making guidance on reproducibility available\nThe project thus helps make:\n\nRecommendations on how to approach reproducibility and structure your research compendium.\nRecommendations on structured ways to reference digital objects.\nRecommendations on how to cite others’ code and how to cite open datasets in your research."
  },
  {
    "objectID": "docs/index.html#cataloguing-all-open-datasets-applicable-to-the-discipline",
    "href": "docs/index.html#cataloguing-all-open-datasets-applicable-to-the-discipline",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Cataloguing all open datasets applicable to the discipline",
    "text": "Cataloguing all open datasets applicable to the discipline\nThe project also makes resources available, such as:\n\nThe Price Statistics Open Data Catalogue.\nGuidance on how the Price Statistics Open Data Catalogue works.\nGuidance on how to register open dataset to the Price Statistics Data Catalogue."
  },
  {
    "objectID": "docs/index.html#footnotes",
    "href": "docs/index.html#footnotes",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Definitions in The The Turing Way for more details.↩︎\nFor instance see sessions by UN ESCAP, UN SIAP, or the University of Luxembourg as example classes.↩︎\nSee Price and Marques (2023) for an overview of RAP to price statistics.↩︎"
  },
  {
    "objectID": "project-content/project-charter.html",
    "href": "project-content/project-charter.html",
    "title": "Project Charter",
    "section": "",
    "text": "Research in the price statistics discipline is not as reproducible as we feel it should be. Most researchers utilize proprietary datasets (for instance internal datasets owned by their NSOs as part of their official work, or purchased datasets that require considerable financial investment for others to acquire). Research is also usually done with proprietary software and is not shared with the research output. The result is that it is challenging to reproduce or replicate trialed methods, as well as to extend and teach (oneself or others) the methods in question.\nThis consequence is far from the intention of researchers, but is a result of the challenges within the discipline making adoption of good practices hard. For instance, it is not easy to find, access, or understand open datasets so that they can be used for research purposes. There is also a lack of discipline-specific guidance on how Findable, Accessible, Interoperable, or Reusable (or FAIR) principles can be applied to projects to make them reproducible.\n\n\n\nThe project aims to instill reproducibility within the discipline by lowering barriers and simplifying processes for adoption of good practices. This overarching goal is aligned with the Principles of Official Statistics, specifically the strengthening of professional standards, scientific principles, and professional ethics."
  },
  {
    "objectID": "project-content/project-charter.html#project-overview",
    "href": "project-content/project-charter.html#project-overview",
    "title": "Project Charter",
    "section": "",
    "text": "Research in the price statistics discipline is not as reproducible as we feel it should be. Most researchers utilize proprietary datasets (for instance internal datasets owned by their NSOs as part of their official work, or purchased datasets that require considerable financial investment for others to acquire). Research is also usually done with proprietary software and is not shared with the research output. The result is that it is challenging to reproduce or replicate trialed methods, as well as to extend and teach (oneself or others) the methods in question.\nThis consequence is far from the intention of researchers, but is a result of the challenges within the discipline making adoption of good practices hard. For instance, it is not easy to find, access, or understand open datasets so that they can be used for research purposes. There is also a lack of discipline-specific guidance on how Findable, Accessible, Interoperable, or Reusable (or FAIR) principles can be applied to projects to make them reproducible."
  },
  {
    "objectID": "project-content/project-charter.html#raison-dêtre-vision-for-the-project",
    "href": "project-content/project-charter.html#raison-dêtre-vision-for-the-project",
    "title": "Project Charter",
    "section": "",
    "text": "The project aims to instill reproducibility within the discipline by lowering barriers and simplifying processes for adoption of good practices. This overarching goal is aligned with the Principles of Official Statistics, specifically the strengthening of professional standards, scientific principles, and professional ethics."
  },
  {
    "objectID": "project-content/project-charter.html#overall-objective-for-the-year-between-may-2025-and-late-april-2026",
    "href": "project-content/project-charter.html#overall-objective-for-the-year-between-may-2025-and-late-april-2026",
    "title": "Project Charter",
    "section": "Overall objective for the year between May 2025 and late April 2026",
    "text": "Overall objective for the year between May 2025 and late April 2026\n\n2025-26 OKRs\n\n\n\n\n\n\nObjective:\nInstill foundational reproducibility principles in methodological and teaching material shared within the discipline, as measured by\n\n\n\n\nKey Result 1:\nDevelopment of foundational guidance on reproducibility to ensure all critical use cases are covered.\n\n\nKey Result 2\nCataloguing of 10 diverse and open datasets to support research with various methods\n\n\nKey Result 3\nReproducibility in Ottawa Group proceedings up by at least N%\n\n\n\nEach KR has an issue that focuses on how to measure it."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "When contributing, post comments and discussion changes you wish to make via Issues.\nFeel free to propose changes by creating Pull Requests. If you don’t have write access, editing a file will create a Fork of this project for you to save your proposed changes to. Submitting a change to a file will write it to a new Branch in your Fork, so you can send a Pull Request."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-contribute",
    "href": "CONTRIBUTING.html#how-to-contribute",
    "title": "Contributing",
    "section": "",
    "text": "When contributing, post comments and discussion changes you wish to make via Issues.\nFeel free to propose changes by creating Pull Requests. If you don’t have write access, editing a file will create a Fork of this project for you to save your proposed changes to. Submitting a change to a file will write it to a new Branch in your Fork, so you can send a Pull Request."
  },
  {
    "objectID": "project-content/meeting-minutes.html",
    "href": "project-content/meeting-minutes.html",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "This document summarizes the meetings of the workstream\n\n\n\n\n\nIntroductions\nDiscussions on problem statement and possible solutions\nIdeas on how to go forward\nWrap up and immediate next steps\n\n\n\n\n\nGroup met with context that reproducibility is poor in price statistics research and we should improve it. The group then decided how to tackle this problem and how to properly scope the objectives to make things achievable. Main challenges currently faced in the discipline included:\n\nResearch is becoming increasingly empirical, hence we need processes to work with data and code to make the research more easily reproducible\nThere are no agreed upon benchmark datasets per se in the discipline to test methods on\nThe discipline will not own most of the datasets as most research is done on either confidential internal datasets owned by NSOs or on proprietary (purchased datasets). The open datasets that exist are not organized or cohesively documented/available.\nThe reason why reproducibility is important is not as inherent/widely communicated as it could be, hence any attempts to solve the technical and processes aspects needs to include this aspect in the communication.\n\nThe group touched on goals to solve these aspects and on processes that can be set up to incentivize reproducibility, including by lowering the complexity and creating an easy ‘on ramp’ to making projects more reproducible (including by cataloging open datasets and showcasing how code can be made reproducible), coordinating with the two bi-annual conference to recommend reproducibility be part of the paper submission process, embedding metadata standards into the data available to make it easier and more standardized when various benchmark datasets are used to evaluate a specific method, etc.\nThe data catalogue for open datasets was seen as a major deliverable to the discipline, and one that needed to be phased. In other words we can set up a ‘proof of concept’ or interim catalogue in a simple way to demonstrate the use case, and later transition to a fuller and more comprehensive catalogue with more resources and infrastructure, potentially hosted by the UN Global Platform. Showcasing the interim solution and broadening the adoption to beyond just price statistics would help make this business case.\nThe outcome of the discussion resolved to target two main deliverables: developing the proof of concept data catalogue, and writing guidance (for instance in the form of a white paper) on how to make projects more reproducible. This target scope was later summarized through our project charter.\n\n\n\n\n\n\n\n\nIntro and discussion on timelines and scope. Confirm cadence of meetings\nQuick discussion on how to track PM activity. We could use something like GitHub, the UN wiki, or some other option\nDiscussion on Objective A. We have two options (.Stat suite and data contract catalogue) for interim data catalogue.\nDiscussion on Objective B. Can we adopt any best practices from the Turning Way and their template repo for price statistics?\nRoundtable\n\n\n\n\n\nGroup discussed objectives for the 2025 CPI Expert Group meeting. It was decided to focus on an interim data catalogue and provide interim guidance at the conference, with the fuller guidance to be developed over the next year.\nGitHub projects was agreed upon as the structure for PM activity\nThe draft data catalogue idea was given the green light to flush out further as our likely implementation of the interim version. Metadata standards should be implemented but it would be hard to use a platform where we don’t own the dataset (such as dominik’s data).\nGuidelines for how to develop reproducible research through git was seen as a good way, with the idea that the guidance we would produce would (a) provide the target state to aim for and (b) summarize maturity levels (similar to RAP maturity levels) that showcase how researchers can start easily and progress over time.\n\n\n\n\n\n\n\n\nDiscussion on the proposed project charter for the project (since merged into main)\nUpdate on the 2 repositories for the project (project repo and interim data catalogue repo) and the mocked up project management approach\nRoundtable\n\n\n\n\n\nTeam discussed scope for 2025 CPI EG presentation. The project plan outlined two milestones, one on data catalogue and one on guidance will be the target. The project structure and use of the GitHub project was summarized.\nThe use of the two repos was discussed. The reproducibility-project repo will host the guidance we develop in whatever format we decide (ideas shared including using quarto to write reproducible papers, or a quarto static site if we will more tend towards creating guidance). price-stats-data-catalogue will host the interim open data catalogue.\nScope of the data contract was firmed up - we could aim to catalogue input datasets that are used to create some experimental indices and version the output datasets (that may be price indices or other artifacts) as part of the repository on github (such as by saving them in data folder and formatting them in tidy data format).\nUse of tools like Zenodo was discussed and will be investigated to mint DOIs - ticket #3\n\n\n\n\n\n\n\n\nDiscussion on format for meeting minutes and how to review/approve the minutes each meeting.\nHow to track materials related to reproducibility but are just references to others’ material (not the overall guidance we will provide).\nWhat format is seen as the best way to deliver guidance on reproducibility? It is best to decide an applicable format and stick with it. Options include writing a paper, using a static site, or using the wiki\n\n\n\n\n\nThe team agreed on keeping notes in this meeting-minutes.md file. The team also agreed to the process:\n\nThe note taker would summarize the meeting and would draft a branch and prepare a Pull Request for the team to review the meeting minutes at the next meeting.\nAt the start of the next meeting, the team would review the PR, would commit any changes/fixes needed. and squash and commit the PR into the main branch to approve the minutes.\n\nThe team agreed to track other materials in other-open-materials.md for now\nThe team discussed the means of how guidance will be provided at the end of the day as a lot of material could be included based on the goals in Objective B of our project. Some possible options:\n\na paper (similar to say the FAIR paper on software to be presented at say 2026 Ottawa Group conference) as the main document for guidance and other materials as supporting (like the catalogue). This could use the quarto manuscript format for example.\na static site (again say a quarto one like this one) as the main means of sharing guidance, but also a short paper for the 2026 OG conference as an offline guide\na set of wiki pages similar to other content made by the Task Team\n\nThe team agreed that a static site (the quarto option) is likely the most appropriate as the site can be expanded and maintained as appropriate, a presentation with a link to the site could be provided at conferences.\nThe team discussed planning for the next 1.5 months. The project roadmap outlines the target timelines. As several issues remain unassigned, the team are encouraged to sign up based on bandwidth.\nRoundtable discussion included:\n\nUpdate on Zenodo - which is an option for uploading and citing data (detail in issue 3). There isn’t a process yet identified for datasets that are not owned by the community and where the owner does not upload it to a repository that mints a DOI.\nUpdate on citing data in PriceIndices R package as it has a DOI. Ability to extract the data without installing the package has to be confirmed.\n\n\n\n\n\n\n\n\n\nReview mock up quarto site for the project, as well as contributing and code of conduct\nReview ongoing work prior to the 2025 CPI Expert Group\nRoundtable\n\n\n\n\n\nTeam discussed the mock up site structure with a home and two main sections, one on how to make your projects reproducible by publishing your code and on using open data.\nThe team also discussed the basic contributing guide and code of conduct.\nThe deliverables to be presented at the 2025 CPI Expert Group were discussed as per the view in the project roadmap.\nAn approach on how to import data from a package was discussed. For instance, how should researchers use data from an R package (such as the PriceIndices package, which has datasets we would like to make list in the interim catalogue) and they wanted to import the data and use it in Python? The team discussed on a phased approach: guidance on how to download the dataset and use it in python from the R ecosystem will be proposed; a longer term approach could be to work with dataset owners to get them to publish it on a repository like Zenodo.\nSupport by the UN Global Platform team for the project was also discussed\n\n\n\n\n\n\n\n\nReview of skeleton data catalogue\nReview of the changes to the project site, specifically how to cite code and how to cite data sections\nReview roadmap and discussion of topics left to finish in this phase of the project\n\n\n\n\n\nThe team discussed the skeleton of the proof of concept data catalogue. The technical process to register new datasets is basically to (1) draft a new yaml file in the datasets/ folder using the datacontract.cli specifications, and then (2) when the PR is accepted (after relevant review) and merged with the main branch, the runner will re-render the catalogue and the dataset will show up.\nThe team discussed next steps. The dataset in #6 is still the third we’d want for presentation at CPI EG.\nThere is a need to differentiate open versus proprietary but popular datasets. Open datasets will be the focus for now with potential for expansion after the conference.\nThe team discussed how to cite datasets and how to cite code topics, and based on the example by the recent Baker et al (2022) FAIR principles for software paper, we decided to go with a nuanced recommendation for now:\n\nif data or code that a research uses exists should be included in the bibliography\ndata or code that is created as part of the paper should be (ideally published to something that mints a DOI) but the links to the dataset or code are included at the end of the paper under “Data availability” and “Code availability” headers.\nThe idea of topics to discuss after the conference was also brought up - the process of creating synthetic datasets.\n\nTo support researchers to structure their code, the team also discussed and endorsed recommending a template RAP.\n\n\n\n\n\n\n\n\nReview of mock-up guidance on the project cite and the template RAP for price index methods.\nDiscussion of the contributing guide for the catalogue\nReview roadmap and discussion of topics left to finish in this phase of the project\n\n\n\n\n\nThe team discussed the flushed out project site with the initial guidance that could be provided during the upcoming CPI Expert Group conference and the mock-up price index RAP.\nThe group particularly focused on the contributing guide for the catalogue and how to deal with various scenarios. Particularly:\n\nHow should we assess and decide what is approved to be registered to the catalogue? The team will for now adopt a group consensus approach of approving new datasets following a discussion during one of the regular team meetings. The approach could be flushed out in more detail once there are several additional datasets to register.\nWhat is the scope for datasets registered to the catalogue? The team agreed that any dataset related to price statistics, whether consumer, producer, or otherwise, could be included - as the methods to be applied are similar, even if datasets and applications are slightly different. For more nuanced cases (such as elementary aggregate data), the justification to include may depend on value for researchers.\n\nNext steps for the group is to prepare for the upcoming CPI Expert Group conference by reviewing the mocked-up content and preparing the presentation material.\n\n\n\n\n\n\n\n\nAdding an “about the team” section in the about page to showcase project members/contributors, similar to the Turing way.\nReview material to be presented during the CPI Expert Group\nDiscussion on how material related to academic classes in price statistics can be positioned to be reproducible\n\n\n\n\n\nThe team discussed and endorsed the about team page, similar to how the Turing way records contributors. Action item for the team is either to contribute to the page directly via PR (see CONTRIBUTING) or coordinate offline.\nHow this guide and the interim catalogue will be presented next week was discussed and approved\nFor academic material, the group discussed possible guidance. Initial thoughts of the group was that while code can be in GitHub and follow compendium type structure, datasets do not need to be registered if they are small training datasets and could just be version controlled in GitHub directly. The policies of the university should take precedence.\nThe group agreed to switch to a 3 week cadence with the next meeting three weeks after the conference.\n\n\n\n\n\n\n\n\nDiscuss OKR based slight restructure of the charter to clarify the direction.\nDiscuss what to recommend as guidance in the call out email when the Ottawa Group organizers do a call for papers\nDiscuss backlog of items to prioritize work for the year (did not finish)\n\n\n\n\n\nThe team discussed and approved the proposed OKR based restructure of the project charter. Key points included how to include administrative activities that aim to disseminate knowledge about the project/our guidance. Final charter and structured milestones can be mocked up and presented at the subsequent meeting.\nThe team discussed how training (provided by workstream 3) could support the project. A notable example was RAP principles, which can be taught in a general way and options to apply them to either research or production could be given at the end. A process flow may also be applicable to support individuals knowing how to open their projects.\nThe team discussed the difference between a researcher developing a new method (in which case open data is best) or a researcher is applying a tried method on their internal data (in which case use of proprietary data is absolutely fine).\nTeam to add PR directly or email Serge to add their contact info to the Project team members section.\n\n\n\n\n\n\n\n\nUpdated project charter and github project.\nPull request on other open examples.\nDiscuss possible style changes to the site\n\n\n\n\n\nThe team discussed and agreed upon an updated project direction for the year. Specifically, the overall vision has been clarified with the goal of instilling of reproducibility practices in the price statistics discipline. Furthermore, a project management appraoch using objective and key results (KRs) that the team aims to achieve per year towards the vision was also set up. Note that the specific KRs may need to be tweaked and will be discussed at subsequent meetings.\nA PR with other examples was discussed. The papers section was discussed specifically - idea is to show papers with ‘packaged’ code as there could be many notable repos mentioned in papers and it could be challenging to be comprehensive. For really notable ones, we could encourage packaging. The section will be further refined before being pushed.\nStyle changes were discussed for the site. We could test using an &lt;iframe&gt; to embed the catalogue into the site. We can also organize how to guides separately from initial introductory content. Site will be mocked up with proposed changes at a later meeting.\n\n\n\n\n\n\n\n\nReview proposed OKR changes to structure goals for the year\nDiscussion on “Open material for price statistics” guide\nDiscussion of an example that could serve as a proprietary to open guide\n\n\n\n\n\nTeam reviewed and agreed to proposed changes to the OKRs with 3 KRs for one objective:\n\nKey challenge of how to measure was discussed. For growth for instance (KR3), its hard to measure as the denominator (if we use a ratio) is tough to agree to (not all conference presentations are research materials). Aiming for a specific number of papers may be easier to estimate.\nNext steps include the discussions on how to measure each Key Result.\n\nTeam discussed the Overview on open material for price statistics page:\n\nArticle is made up of 3 sections: generic (software for official statistics), software specifically for price statistics (needs to be on hosted on CRAN etc.), and older/inactive projects. Actual packages are included, i.e. to make the list, it needs to be installable (not code used to produce analysis).\nThis should be a living document - would fall under longer term task team maintenance.\nInfeasible to keep package owners notified that we included their package on the list. CRAN and other repos already let owners know who uses their packages.\nCaveats around using packages, esp. old/outdated ones on the page (e.g. with a disclaimer) makes the list useful.\nThe list is intended to advocate best practice of making software citable to help increase awareness\nTeam to review draft PR by end of week\n\nTeam discussed the GEKS decomposition code that was published alongside Liu (2025) Practical Decompositions of mean-spliced on published indices. The paper was initially written with internal data and internal code in mind but converted to use open data and published its code alongside the paper. Thus, the example could use as a good use case as guidance (say as a ‘call to action’). Group agreed on the value and will look to return to the article to decide on best approach.\n\n\n\n\n\n\n\n\nUI enhancements to the project site\nRequest to steering committees of the CPI Expert Group and Ottawa Group conferences about referencing our material\nDiscussion about how to measure the KRs\n\n\n\n\n\nTeam discussed and approved UI enhancements to the project site, including (1) a cleaner structure for guides grouped first by a section on how to get started, three sections for targeted guides on research compendium, citation, and data topics, and a section on other open material, and (2) a cleaner data catalogue section that includes embedding the catalogue itself into the site as an &lt;iframe&gt;.\nThe team discussed and approved the email to request that the two main price statistics conferences reference our material:\n\nSpecifically the goal would be for the conference call for papers to (1) reference our project and reproducibility guidance, (2) reference the data catalogue, and (3) invite participants to reach out to us to propose additional open datasets.\nAs some visitors to the site may still be confused, the team decided on a streamlined approach on how we could help. Issue 64 to track the work .\n\nTeam started discussing how to measure the KRs to meet this year’s objective to have clear and set agreement on the definition of success. Proposed approach on how to measure KR1, KR2, KR3 are mocked up and will be discussed in greater detail during a further couple meetings.\n\n\n\n\n\n\n\n\nTurvey datasets as a potential to add to the catalogue\nContinue scoring KRs discussion\n\n\n\n\n\nTeam discussed the Turvey dataset and agreed that its highly applicable and should be registered to the catalogue. Next steps:\n\nRegister the dataset as is and reference versions on GitHub so that users can get it easily. Team to review the record at subsequent meeting.\nCoordiante with IWGPS/ILO to clarify license and simplify access. Specifically, ask the ILO to make a csv and publish to zenodo.\n\nTeam discussed datasets to prioritize to help develop examples for the guides – especially as guides would help those we collaborate with (like the ILO). The NZ dataset, PriceIndices ones, or synthetic ones were discussed with existing ones likely the easiest.\nTeam discussed scoring approach to registering datasets Key Result and target for the year. A bit more work is needed to clearly flush out steps of the registry and zenodo process in the contributing section of the catalogue prior to confirming the methodology. Finalize and discuss next meeting\n\n\n\n\n\n\n\n\nDiscuss information to consider when registering a dataset using Turvey record as example\nDiscuss Italian web scraped food dataset as consideration to the catalogue\n\n\n\n\n\nTeam discussed the information necessary to consider a dataset fully registered using the modified Turvey dataset record as an example.\n\nThree considerations were discussed as key to aim for: hosting the dataset (ideally in a data repository like Zenodo); citation (with proper instructions); and cataloguing (i.e. adding all relevant information).\nThe team discussed if the version of the dataset catalogue record mattered and concluded that it does not but should be tied to what the data repository uses. If datasets change (such as if its regularly updated and expanded), the catalogue record would reflect the latest version that was assessed and catalogued.\nIdeally a method or data paper is available to link when we catalogue the dataset.\n\nA newly proposed web scraped dataset on food prices from several retailers in Italy was discussed and agreed as applicable to register by the group.\n\n\n\n\n\n\n\n\nUpdates on progress registering Turvey and next steps\nDiscuss if we write a data exploration blog on the project site for new datasets to the catalogue\nDiscuss idea of registering COICOP2018 to the catalogue\n\n\n\n\n\nFuther to last meetings’ conversation on datasets like the Turvey dataset, the team agreed that the catalogue record should point to the easiest to use version (wherever it happens to be) as an interim measure while the team works with the dataset owner to move it to a data repository and save it in an open-source format.\nBuilding on last meetings’ conversations – the team considered whether we should write an empirical blog on the ‘Blog’ section of the project site. Benefits of a data blog include:\n\nA walk through of the dataset (alike to a detailed data paper) with example code to visually demonstrate key features like product churn, geographic coverage, etc.\nThe blog could be written jointly with the dataset owners, thereby incentivizing participation by the community.\n\nThe team proposed crating a maturity level for dataset registry to the catalogue, with the ‘gold’ level to include a dataset exploration blog.\nCOICOP2018 was discussed and was not seen as a dataset of sufficient value to register to the catalogue. Taxonomies were seen as already well documented and available on organizational sites, meaning the catalogue would not add much value. Furthermore, registering one would imply that all should be registered, which would could add a large number of low-value added records to the catalogue.\n\n\n\n\n\n\n\n\nMaturity model and labels for datasets registered to the catalogue\nMaturity model for reproducibility and getting started article\nArticles to prioritize that should be at least draft so that our colleagues submitting absracts to the Ottawa Group call for papers can reference them\n\n\n\n\n\nA rough maturity model was discussed for cataloging datasets, with level 1 as an acceptable starting poing and level 3 as the top level with ‘bells and whistles’. Labels to categorized datasets was also introduced. Data type (of the main types of alterantive data) and topic the dataset covers (as this would drive application of the dataset towards testing applicable methods) was agreed as a good starting point. Sections will be finalized for discussed at the next meeting.\nAs guidance for researchers making their projects reproducible, the value of having a clear maturity level checklist was discussed to expand on general getting started guide. It was agreed that a checklist was of value to ensure that it was discipline specific and more approachable to the 3 NHS leves of RAP. Material will be finalized for discussion at the next meeting.\nIt was agreed that drafting How to structure a research compendium, Using open data (or opening data), and how to work with sensitive or proprietary data are valuable articles to prioritize.\n\n\n\n\n\n\n\n\nFinalized getting started guidance and the maturity model for reproducibility\nUpdated process to propose datasets to the catalogue and the maturity model for how well flushed out a dataset can be\nUpdates to other draft/WIP sections to ensure that the site does not look empty\nValidate topics for next meeting\n\n\n\n\n\nThe getting started guide with a maturity model for reproducibility has been drafted and summarized to the group for review. Its purpose is serve as a gentler intro to RAP. Team to review the draft over the coming weeks async.\nGuidance on how to propose ideas to the catalogue was flushed out, along side with standardized dataset labels that can serve as a starting point, and a few additional FAQs. Creation of an issue and PR template to simplify the process still in the works (#25). Team to review over the coming weeks async.\nInital draft mocked up to research compendium deep dive, as well as WIP callouts flushed out (the metadata one is detailed) so that researchers drafting abstracts for the Ottawa group who visit the site will see a semi put together product.\nNew Zealand modified scanner consumer electronics dataset to be mocked for discussion at a future meeting."
  },
  {
    "objectID": "project-content/meeting-minutes.html#kick-off",
    "href": "project-content/meeting-minutes.html#kick-off",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Introductions\nDiscussions on problem statement and possible solutions\nIdeas on how to go forward\nWrap up and immediate next steps\n\n\n\n\n\nGroup met with context that reproducibility is poor in price statistics research and we should improve it. The group then decided how to tackle this problem and how to properly scope the objectives to make things achievable. Main challenges currently faced in the discipline included:\n\nResearch is becoming increasingly empirical, hence we need processes to work with data and code to make the research more easily reproducible\nThere are no agreed upon benchmark datasets per se in the discipline to test methods on\nThe discipline will not own most of the datasets as most research is done on either confidential internal datasets owned by NSOs or on proprietary (purchased datasets). The open datasets that exist are not organized or cohesively documented/available.\nThe reason why reproducibility is important is not as inherent/widely communicated as it could be, hence any attempts to solve the technical and processes aspects needs to include this aspect in the communication.\n\nThe group touched on goals to solve these aspects and on processes that can be set up to incentivize reproducibility, including by lowering the complexity and creating an easy ‘on ramp’ to making projects more reproducible (including by cataloging open datasets and showcasing how code can be made reproducible), coordinating with the two bi-annual conference to recommend reproducibility be part of the paper submission process, embedding metadata standards into the data available to make it easier and more standardized when various benchmark datasets are used to evaluate a specific method, etc.\nThe data catalogue for open datasets was seen as a major deliverable to the discipline, and one that needed to be phased. In other words we can set up a ‘proof of concept’ or interim catalogue in a simple way to demonstrate the use case, and later transition to a fuller and more comprehensive catalogue with more resources and infrastructure, potentially hosted by the UN Global Platform. Showcasing the interim solution and broadening the adoption to beyond just price statistics would help make this business case.\nThe outcome of the discussion resolved to target two main deliverables: developing the proof of concept data catalogue, and writing guidance (for instance in the form of a white paper) on how to make projects more reproducible. This target scope was later summarized through our project charter."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section",
    "href": "project-content/meeting-minutes.html#section",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Intro and discussion on timelines and scope. Confirm cadence of meetings\nQuick discussion on how to track PM activity. We could use something like GitHub, the UN wiki, or some other option\nDiscussion on Objective A. We have two options (.Stat suite and data contract catalogue) for interim data catalogue.\nDiscussion on Objective B. Can we adopt any best practices from the Turning Way and their template repo for price statistics?\nRoundtable\n\n\n\n\n\nGroup discussed objectives for the 2025 CPI Expert Group meeting. It was decided to focus on an interim data catalogue and provide interim guidance at the conference, with the fuller guidance to be developed over the next year.\nGitHub projects was agreed upon as the structure for PM activity\nThe draft data catalogue idea was given the green light to flush out further as our likely implementation of the interim version. Metadata standards should be implemented but it would be hard to use a platform where we don’t own the dataset (such as dominik’s data).\nGuidelines for how to develop reproducible research through git was seen as a good way, with the idea that the guidance we would produce would (a) provide the target state to aim for and (b) summarize maturity levels (similar to RAP maturity levels) that showcase how researchers can start easily and progress over time."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-1",
    "href": "project-content/meeting-minutes.html#section-1",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Discussion on the proposed project charter for the project (since merged into main)\nUpdate on the 2 repositories for the project (project repo and interim data catalogue repo) and the mocked up project management approach\nRoundtable\n\n\n\n\n\nTeam discussed scope for 2025 CPI EG presentation. The project plan outlined two milestones, one on data catalogue and one on guidance will be the target. The project structure and use of the GitHub project was summarized.\nThe use of the two repos was discussed. The reproducibility-project repo will host the guidance we develop in whatever format we decide (ideas shared including using quarto to write reproducible papers, or a quarto static site if we will more tend towards creating guidance). price-stats-data-catalogue will host the interim open data catalogue.\nScope of the data contract was firmed up - we could aim to catalogue input datasets that are used to create some experimental indices and version the output datasets (that may be price indices or other artifacts) as part of the repository on github (such as by saving them in data folder and formatting them in tidy data format).\nUse of tools like Zenodo was discussed and will be investigated to mint DOIs - ticket #3"
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-2",
    "href": "project-content/meeting-minutes.html#section-2",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Discussion on format for meeting minutes and how to review/approve the minutes each meeting.\nHow to track materials related to reproducibility but are just references to others’ material (not the overall guidance we will provide).\nWhat format is seen as the best way to deliver guidance on reproducibility? It is best to decide an applicable format and stick with it. Options include writing a paper, using a static site, or using the wiki\n\n\n\n\n\nThe team agreed on keeping notes in this meeting-minutes.md file. The team also agreed to the process:\n\nThe note taker would summarize the meeting and would draft a branch and prepare a Pull Request for the team to review the meeting minutes at the next meeting.\nAt the start of the next meeting, the team would review the PR, would commit any changes/fixes needed. and squash and commit the PR into the main branch to approve the minutes.\n\nThe team agreed to track other materials in other-open-materials.md for now\nThe team discussed the means of how guidance will be provided at the end of the day as a lot of material could be included based on the goals in Objective B of our project. Some possible options:\n\na paper (similar to say the FAIR paper on software to be presented at say 2026 Ottawa Group conference) as the main document for guidance and other materials as supporting (like the catalogue). This could use the quarto manuscript format for example.\na static site (again say a quarto one like this one) as the main means of sharing guidance, but also a short paper for the 2026 OG conference as an offline guide\na set of wiki pages similar to other content made by the Task Team\n\nThe team agreed that a static site (the quarto option) is likely the most appropriate as the site can be expanded and maintained as appropriate, a presentation with a link to the site could be provided at conferences.\nThe team discussed planning for the next 1.5 months. The project roadmap outlines the target timelines. As several issues remain unassigned, the team are encouraged to sign up based on bandwidth.\nRoundtable discussion included:\n\nUpdate on Zenodo - which is an option for uploading and citing data (detail in issue 3). There isn’t a process yet identified for datasets that are not owned by the community and where the owner does not upload it to a repository that mints a DOI.\nUpdate on citing data in PriceIndices R package as it has a DOI. Ability to extract the data without installing the package has to be confirmed."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-3",
    "href": "project-content/meeting-minutes.html#section-3",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Review mock up quarto site for the project, as well as contributing and code of conduct\nReview ongoing work prior to the 2025 CPI Expert Group\nRoundtable\n\n\n\n\n\nTeam discussed the mock up site structure with a home and two main sections, one on how to make your projects reproducible by publishing your code and on using open data.\nThe team also discussed the basic contributing guide and code of conduct.\nThe deliverables to be presented at the 2025 CPI Expert Group were discussed as per the view in the project roadmap.\nAn approach on how to import data from a package was discussed. For instance, how should researchers use data from an R package (such as the PriceIndices package, which has datasets we would like to make list in the interim catalogue) and they wanted to import the data and use it in Python? The team discussed on a phased approach: guidance on how to download the dataset and use it in python from the R ecosystem will be proposed; a longer term approach could be to work with dataset owners to get them to publish it on a repository like Zenodo.\nSupport by the UN Global Platform team for the project was also discussed"
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-4",
    "href": "project-content/meeting-minutes.html#section-4",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Review of skeleton data catalogue\nReview of the changes to the project site, specifically how to cite code and how to cite data sections\nReview roadmap and discussion of topics left to finish in this phase of the project\n\n\n\n\n\nThe team discussed the skeleton of the proof of concept data catalogue. The technical process to register new datasets is basically to (1) draft a new yaml file in the datasets/ folder using the datacontract.cli specifications, and then (2) when the PR is accepted (after relevant review) and merged with the main branch, the runner will re-render the catalogue and the dataset will show up.\nThe team discussed next steps. The dataset in #6 is still the third we’d want for presentation at CPI EG.\nThere is a need to differentiate open versus proprietary but popular datasets. Open datasets will be the focus for now with potential for expansion after the conference.\nThe team discussed how to cite datasets and how to cite code topics, and based on the example by the recent Baker et al (2022) FAIR principles for software paper, we decided to go with a nuanced recommendation for now:\n\nif data or code that a research uses exists should be included in the bibliography\ndata or code that is created as part of the paper should be (ideally published to something that mints a DOI) but the links to the dataset or code are included at the end of the paper under “Data availability” and “Code availability” headers.\nThe idea of topics to discuss after the conference was also brought up - the process of creating synthetic datasets.\n\nTo support researchers to structure their code, the team also discussed and endorsed recommending a template RAP."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-5",
    "href": "project-content/meeting-minutes.html#section-5",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Review of mock-up guidance on the project cite and the template RAP for price index methods.\nDiscussion of the contributing guide for the catalogue\nReview roadmap and discussion of topics left to finish in this phase of the project\n\n\n\n\n\nThe team discussed the flushed out project site with the initial guidance that could be provided during the upcoming CPI Expert Group conference and the mock-up price index RAP.\nThe group particularly focused on the contributing guide for the catalogue and how to deal with various scenarios. Particularly:\n\nHow should we assess and decide what is approved to be registered to the catalogue? The team will for now adopt a group consensus approach of approving new datasets following a discussion during one of the regular team meetings. The approach could be flushed out in more detail once there are several additional datasets to register.\nWhat is the scope for datasets registered to the catalogue? The team agreed that any dataset related to price statistics, whether consumer, producer, or otherwise, could be included - as the methods to be applied are similar, even if datasets and applications are slightly different. For more nuanced cases (such as elementary aggregate data), the justification to include may depend on value for researchers.\n\nNext steps for the group is to prepare for the upcoming CPI Expert Group conference by reviewing the mocked-up content and preparing the presentation material."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-6",
    "href": "project-content/meeting-minutes.html#section-6",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Adding an “about the team” section in the about page to showcase project members/contributors, similar to the Turing way.\nReview material to be presented during the CPI Expert Group\nDiscussion on how material related to academic classes in price statistics can be positioned to be reproducible\n\n\n\n\n\nThe team discussed and endorsed the about team page, similar to how the Turing way records contributors. Action item for the team is either to contribute to the page directly via PR (see CONTRIBUTING) or coordinate offline.\nHow this guide and the interim catalogue will be presented next week was discussed and approved\nFor academic material, the group discussed possible guidance. Initial thoughts of the group was that while code can be in GitHub and follow compendium type structure, datasets do not need to be registered if they are small training datasets and could just be version controlled in GitHub directly. The policies of the university should take precedence.\nThe group agreed to switch to a 3 week cadence with the next meeting three weeks after the conference."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-7",
    "href": "project-content/meeting-minutes.html#section-7",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Discuss OKR based slight restructure of the charter to clarify the direction.\nDiscuss what to recommend as guidance in the call out email when the Ottawa Group organizers do a call for papers\nDiscuss backlog of items to prioritize work for the year (did not finish)\n\n\n\n\n\nThe team discussed and approved the proposed OKR based restructure of the project charter. Key points included how to include administrative activities that aim to disseminate knowledge about the project/our guidance. Final charter and structured milestones can be mocked up and presented at the subsequent meeting.\nThe team discussed how training (provided by workstream 3) could support the project. A notable example was RAP principles, which can be taught in a general way and options to apply them to either research or production could be given at the end. A process flow may also be applicable to support individuals knowing how to open their projects.\nThe team discussed the difference between a researcher developing a new method (in which case open data is best) or a researcher is applying a tried method on their internal data (in which case use of proprietary data is absolutely fine).\nTeam to add PR directly or email Serge to add their contact info to the Project team members section."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-8",
    "href": "project-content/meeting-minutes.html#section-8",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Updated project charter and github project.\nPull request on other open examples.\nDiscuss possible style changes to the site\n\n\n\n\n\nThe team discussed and agreed upon an updated project direction for the year. Specifically, the overall vision has been clarified with the goal of instilling of reproducibility practices in the price statistics discipline. Furthermore, a project management appraoch using objective and key results (KRs) that the team aims to achieve per year towards the vision was also set up. Note that the specific KRs may need to be tweaked and will be discussed at subsequent meetings.\nA PR with other examples was discussed. The papers section was discussed specifically - idea is to show papers with ‘packaged’ code as there could be many notable repos mentioned in papers and it could be challenging to be comprehensive. For really notable ones, we could encourage packaging. The section will be further refined before being pushed.\nStyle changes were discussed for the site. We could test using an &lt;iframe&gt; to embed the catalogue into the site. We can also organize how to guides separately from initial introductory content. Site will be mocked up with proposed changes at a later meeting."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-9",
    "href": "project-content/meeting-minutes.html#section-9",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Review proposed OKR changes to structure goals for the year\nDiscussion on “Open material for price statistics” guide\nDiscussion of an example that could serve as a proprietary to open guide\n\n\n\n\n\nTeam reviewed and agreed to proposed changes to the OKRs with 3 KRs for one objective:\n\nKey challenge of how to measure was discussed. For growth for instance (KR3), its hard to measure as the denominator (if we use a ratio) is tough to agree to (not all conference presentations are research materials). Aiming for a specific number of papers may be easier to estimate.\nNext steps include the discussions on how to measure each Key Result.\n\nTeam discussed the Overview on open material for price statistics page:\n\nArticle is made up of 3 sections: generic (software for official statistics), software specifically for price statistics (needs to be on hosted on CRAN etc.), and older/inactive projects. Actual packages are included, i.e. to make the list, it needs to be installable (not code used to produce analysis).\nThis should be a living document - would fall under longer term task team maintenance.\nInfeasible to keep package owners notified that we included their package on the list. CRAN and other repos already let owners know who uses their packages.\nCaveats around using packages, esp. old/outdated ones on the page (e.g. with a disclaimer) makes the list useful.\nThe list is intended to advocate best practice of making software citable to help increase awareness\nTeam to review draft PR by end of week\n\nTeam discussed the GEKS decomposition code that was published alongside Liu (2025) Practical Decompositions of mean-spliced on published indices. The paper was initially written with internal data and internal code in mind but converted to use open data and published its code alongside the paper. Thus, the example could use as a good use case as guidance (say as a ‘call to action’). Group agreed on the value and will look to return to the article to decide on best approach."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-10",
    "href": "project-content/meeting-minutes.html#section-10",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "UI enhancements to the project site\nRequest to steering committees of the CPI Expert Group and Ottawa Group conferences about referencing our material\nDiscussion about how to measure the KRs\n\n\n\n\n\nTeam discussed and approved UI enhancements to the project site, including (1) a cleaner structure for guides grouped first by a section on how to get started, three sections for targeted guides on research compendium, citation, and data topics, and a section on other open material, and (2) a cleaner data catalogue section that includes embedding the catalogue itself into the site as an &lt;iframe&gt;.\nThe team discussed and approved the email to request that the two main price statistics conferences reference our material:\n\nSpecifically the goal would be for the conference call for papers to (1) reference our project and reproducibility guidance, (2) reference the data catalogue, and (3) invite participants to reach out to us to propose additional open datasets.\nAs some visitors to the site may still be confused, the team decided on a streamlined approach on how we could help. Issue 64 to track the work .\n\nTeam started discussing how to measure the KRs to meet this year’s objective to have clear and set agreement on the definition of success. Proposed approach on how to measure KR1, KR2, KR3 are mocked up and will be discussed in greater detail during a further couple meetings."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-11",
    "href": "project-content/meeting-minutes.html#section-11",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Turvey datasets as a potential to add to the catalogue\nContinue scoring KRs discussion\n\n\n\n\n\nTeam discussed the Turvey dataset and agreed that its highly applicable and should be registered to the catalogue. Next steps:\n\nRegister the dataset as is and reference versions on GitHub so that users can get it easily. Team to review the record at subsequent meeting.\nCoordiante with IWGPS/ILO to clarify license and simplify access. Specifically, ask the ILO to make a csv and publish to zenodo.\n\nTeam discussed datasets to prioritize to help develop examples for the guides – especially as guides would help those we collaborate with (like the ILO). The NZ dataset, PriceIndices ones, or synthetic ones were discussed with existing ones likely the easiest.\nTeam discussed scoring approach to registering datasets Key Result and target for the year. A bit more work is needed to clearly flush out steps of the registry and zenodo process in the contributing section of the catalogue prior to confirming the methodology. Finalize and discuss next meeting"
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-12",
    "href": "project-content/meeting-minutes.html#section-12",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Discuss information to consider when registering a dataset using Turvey record as example\nDiscuss Italian web scraped food dataset as consideration to the catalogue\n\n\n\n\n\nTeam discussed the information necessary to consider a dataset fully registered using the modified Turvey dataset record as an example.\n\nThree considerations were discussed as key to aim for: hosting the dataset (ideally in a data repository like Zenodo); citation (with proper instructions); and cataloguing (i.e. adding all relevant information).\nThe team discussed if the version of the dataset catalogue record mattered and concluded that it does not but should be tied to what the data repository uses. If datasets change (such as if its regularly updated and expanded), the catalogue record would reflect the latest version that was assessed and catalogued.\nIdeally a method or data paper is available to link when we catalogue the dataset.\n\nA newly proposed web scraped dataset on food prices from several retailers in Italy was discussed and agreed as applicable to register by the group."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-13",
    "href": "project-content/meeting-minutes.html#section-13",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Updates on progress registering Turvey and next steps\nDiscuss if we write a data exploration blog on the project site for new datasets to the catalogue\nDiscuss idea of registering COICOP2018 to the catalogue\n\n\n\n\n\nFuther to last meetings’ conversation on datasets like the Turvey dataset, the team agreed that the catalogue record should point to the easiest to use version (wherever it happens to be) as an interim measure while the team works with the dataset owner to move it to a data repository and save it in an open-source format.\nBuilding on last meetings’ conversations – the team considered whether we should write an empirical blog on the ‘Blog’ section of the project site. Benefits of a data blog include:\n\nA walk through of the dataset (alike to a detailed data paper) with example code to visually demonstrate key features like product churn, geographic coverage, etc.\nThe blog could be written jointly with the dataset owners, thereby incentivizing participation by the community.\n\nThe team proposed crating a maturity level for dataset registry to the catalogue, with the ‘gold’ level to include a dataset exploration blog.\nCOICOP2018 was discussed and was not seen as a dataset of sufficient value to register to the catalogue. Taxonomies were seen as already well documented and available on organizational sites, meaning the catalogue would not add much value. Furthermore, registering one would imply that all should be registered, which would could add a large number of low-value added records to the catalogue."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-14",
    "href": "project-content/meeting-minutes.html#section-14",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Maturity model and labels for datasets registered to the catalogue\nMaturity model for reproducibility and getting started article\nArticles to prioritize that should be at least draft so that our colleagues submitting absracts to the Ottawa Group call for papers can reference them\n\n\n\n\n\nA rough maturity model was discussed for cataloging datasets, with level 1 as an acceptable starting poing and level 3 as the top level with ‘bells and whistles’. Labels to categorized datasets was also introduced. Data type (of the main types of alterantive data) and topic the dataset covers (as this would drive application of the dataset towards testing applicable methods) was agreed as a good starting point. Sections will be finalized for discussed at the next meeting.\nAs guidance for researchers making their projects reproducible, the value of having a clear maturity level checklist was discussed to expand on general getting started guide. It was agreed that a checklist was of value to ensure that it was discipline specific and more approachable to the 3 NHS leves of RAP. Material will be finalized for discussion at the next meeting.\nIt was agreed that drafting How to structure a research compendium, Using open data (or opening data), and how to work with sensitive or proprietary data are valuable articles to prioritize."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-15",
    "href": "project-content/meeting-minutes.html#section-15",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Finalized getting started guidance and the maturity model for reproducibility\nUpdated process to propose datasets to the catalogue and the maturity model for how well flushed out a dataset can be\nUpdates to other draft/WIP sections to ensure that the site does not look empty\nValidate topics for next meeting\n\n\n\n\n\nThe getting started guide with a maturity model for reproducibility has been drafted and summarized to the group for review. Its purpose is serve as a gentler intro to RAP. Team to review the draft over the coming weeks async.\nGuidance on how to propose ideas to the catalogue was flushed out, along side with standardized dataset labels that can serve as a starting point, and a few additional FAQs. Creation of an issue and PR template to simplify the process still in the works (#25). Team to review over the coming weeks async.\nInital draft mocked up to research compendium deep dive, as well as WIP callouts flushed out (the metadata one is detailed) so that researchers drafting abstracts for the Ottawa group who visit the site will see a semi put together product.\nNew Zealand modified scanner consumer electronics dataset to be mocked for discussion at a future meeting."
  },
  {
    "objectID": "project-content/other-open-materials.html",
    "href": "project-content/other-open-materials.html",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "The following list is a mini-compenidum of other materials that may be of interest to reseachers in the prices statistics discipline. The list is meant to be live and up to date, hence if you see any inaccuracies, a broken link, or something missing - please feel free to add!\n\n\n\nAwesome list of official statistics software\nReferences in JOSS paper\nSteve’s stars\n\n\n\n\nhicp package (Eurostat) - provides functions to access and work with HICP data (price indices and weights) from Eurostat’s public database\n‘PriceIndices’ package\nIndexNumR package\npiar package\nCRAN task view for official statistics\n\n\n\n\n\nPriceIndexCalc package - a partially working package to calculate price indices.\n\n\n\n\n\ndff - a repo on Dominick’s Finer Foods showcasing the dataset by Jens Mehrhoff\n\n\n\n\n\n\n\n\nThe Turing Way - an open science/collaboration project\nreproducible-project-template - repo template (i.e cookie cutter template)\n\n\n\n\n\nRAP Companion, by Matthew Gregory and Matthew Upson shows much of the original RAP ideas (for R)\nUdemy course on RAP with R\nReproducible Analytical Pipelines or RAP (NHS) site\nESCAP training on RAP with web scraping application for price statistics"
  },
  {
    "objectID": "project-content/other-open-materials.html#packages-applicable-to-price-statistics",
    "href": "project-content/other-open-materials.html#packages-applicable-to-price-statistics",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "Awesome list of official statistics software\nReferences in JOSS paper\nSteve’s stars\n\n\n\n\nhicp package (Eurostat) - provides functions to access and work with HICP data (price indices and weights) from Eurostat’s public database\n‘PriceIndices’ package\nIndexNumR package\npiar package\nCRAN task view for official statistics\n\n\n\n\n\nPriceIndexCalc package - a partially working package to calculate price indices.\n\n\n\n\n\ndff - a repo on Dominick’s Finer Foods showcasing the dataset by Jens Mehrhoff"
  },
  {
    "objectID": "project-content/other-open-materials.html#reproducibility-resources",
    "href": "project-content/other-open-materials.html#reproducibility-resources",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "The Turing Way - an open science/collaboration project\nreproducible-project-template - repo template (i.e cookie cutter template)\n\n\n\n\n\nRAP Companion, by Matthew Gregory and Matthew Upson shows much of the original RAP ideas (for R)\nUdemy course on RAP with R\nReproducible Analytical Pipelines or RAP (NHS) site\nESCAP training on RAP with web scraping application for price statistics"
  },
  {
    "objectID": "docs/faq.html",
    "href": "docs/faq.html",
    "title": "Frequently asked questions",
    "section": "",
    "text": "TipHelp us fill this page!\n\n\n\nHave an idea for a common topic to add to this FAQ? Give us a shout! Submit an issue to the project and tag the the issue using the “question” label."
  },
  {
    "objectID": "docs/faq.html#option-1-submit-an-issue-in-the-repo",
    "href": "docs/faq.html#option-1-submit-an-issue-in-the-repo",
    "title": "Frequently asked questions",
    "section": "Option 1: Submit an issue in the repo",
    "text": "Option 1: Submit an issue in the repo\nCreate a new issue under the project with your question and the team will get back to you as soon as we can!"
  },
  {
    "objectID": "docs/faq.html#option-2-contact-us-directly",
    "href": "docs/faq.html#option-2-contact-us-directly",
    "title": "Frequently asked questions",
    "section": "Option 2: Contact us directly",
    "text": "Option 2: Contact us directly\nCheck out the about the team section for our info. The project lead’s email will be included if you wish to use it."
  },
  {
    "objectID": "docs/catalogue/catalogue.html",
    "href": "docs/catalogue/catalogue.html",
    "title": "Browse the catalogue",
    "section": "",
    "text": "TipThe catalogue is actually a standalone site\n\n\n\nThis page integrates the catalogue into the page for simplicity—however if you wish to browse the catalogue directly outside this site, check out its site here.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Catalogue",
      "Browse the catalogue"
    ]
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About this project",
    "section": "",
    "text": "This project is led by Workstream 5 of the UN Task Team on Scanner data, part of the UN Committee of Experts on Big Data and Data Science for Official Statistics (UN-CEBD)."
  },
  {
    "objectID": "docs/about.html#what-this-project-aims-to-do",
    "href": "docs/about.html#what-this-project-aims-to-do",
    "title": "About this project",
    "section": "What this project aims to do",
    "text": "What this project aims to do\nResearch in the price statistics discipline is not as reproducible as it could be. Most researchers utilize proprietary datasets and don’t publish the code alongside the research so that a specific methodology or finding can be easily reproduced. The idea of this project is to help in both topics! We aim to:\n\nProvide clear and approachable guidance on how researchers in price statistics can make their projects reproducible (including by learning new skills, by understanding how to publish and cite code, and many other topics).\nSupport cataloging open datasets that can be used as benchmarks to use for research purposes. The idea is to have all findings trialed and demonstrated on open benchmark datasets.\n\nThe project also builds on great efforts of early promoters of open science in the discipline!1 We are aiming to continue to build momentum and make it easier for researchers to work openly!"
  },
  {
    "objectID": "docs/about.html#what-this-project-does-not-aim-to-do",
    "href": "docs/about.html#what-this-project-does-not-aim-to-do",
    "title": "About this project",
    "section": "What this project does not aim to do",
    "text": "What this project does not aim to do\n\n\n\n\n\n\nWarningThis section is still a WIP\n\n\n\nNot all team members are listed, we will update this section soon.\n\n\nAs there is a lot of great guidance on the topic already, including The Turing Way, Reproducible Analytical Pipelines or RAP, and many more resources—the idea is to distill key information for the price statistics community, not to create a new standard."
  },
  {
    "objectID": "docs/about.html#serge-goussev",
    "href": "docs/about.html#serge-goussev",
    "title": "About this project",
    "section": "Serge Goussev",
    "text": "Serge Goussev\n\nRole: Workstream lead (2024 - current)\nGitHub id: sergegoussev\nEmail: serge.goussev@statcan.gc.ca"
  },
  {
    "objectID": "docs/about.html#steve-martin",
    "href": "docs/about.html#steve-martin",
    "title": "About this project",
    "section": "Steve Martin",
    "text": "Steve Martin\n\nRole: Contributor (2024 - current)\nGitHub id: marberts"
  },
  {
    "objectID": "docs/about.html#claude-lamboray",
    "href": "docs/about.html#claude-lamboray",
    "title": "About this project",
    "section": "Claude Lamboray",
    "text": "Claude Lamboray\n\nRole: Contributor (2024 - current)"
  },
  {
    "objectID": "docs/about.html#christophe-bontemps",
    "href": "docs/about.html#christophe-bontemps",
    "title": "About this project",
    "section": "Christophe Bontemps",
    "text": "Christophe Bontemps\n\nRole: Contributor (2024 - current)"
  },
  {
    "objectID": "docs/about.html#tanya-flower",
    "href": "docs/about.html#tanya-flower",
    "title": "About this project",
    "section": "Tanya Flower",
    "text": "Tanya Flower\n\nRole: Contributor (2024 - current)"
  },
  {
    "objectID": "docs/about.html#ben-hillman",
    "href": "docs/about.html#ben-hillman",
    "title": "About this project",
    "section": "Ben Hillman",
    "text": "Ben Hillman\n\nRole: Contributor (2024 - current)"
  },
  {
    "objectID": "docs/about.html#caroline-white",
    "href": "docs/about.html#caroline-white",
    "title": "About this project",
    "section": "Caroline White",
    "text": "Caroline White\n\nRole: Contributor (2024 - current)\nGitHub id: carolinewsc"
  },
  {
    "objectID": "docs/about.html#jens-mehrhoff",
    "href": "docs/about.html#jens-mehrhoff",
    "title": "About this project",
    "section": "Jens Mehrhoff",
    "text": "Jens Mehrhoff\n\nRole: Contributor (2024 - current)\nEmail: jens.mehrhoff@bundesbank.de"
  },
  {
    "objectID": "docs/about.html#footnotes",
    "href": "docs/about.html#footnotes",
    "title": "About this project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotable early efforts include but are not limited to Frances Krsinich publishing the FEWS package with open data in 2018, Jens Mehrhoff showcasing the open Dominick’s Finer Foods scanner dataset in 2019, Erwin Dielert and Chihiro Shimizu opening a dataset of japanese laptop sales, or Jacek Białek including datasets with the PriceIndices R package.↩︎"
  },
  {
    "objectID": "docs/reproducibility-guidance/guides-summary.html",
    "href": "docs/reproducibility-guidance/guides-summary.html",
    "title": "Guides and how-tos",
    "section": "",
    "text": "Find guides on topics that you specifically need\n\nHow to create and structure a research compendium\n\n\nHow to cite objects in your paper\n\n\nWhat to do about data\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/synthetic-data.html",
    "href": "docs/reproducibility-guidance/howtos/synthetic-data.html",
    "title": "How to approach synthetic data",
    "section": "",
    "text": "CautionWIP\n\n\n\nThis page is still in the works. Possible topics (see #43 for more info) to include:\n\nCommon packages for making synthetic data. You could include these in research compendium and others could recreate your data.\nTo publish synthetic data or not (to say Zenodo)\nModifying real data so that it can be shared.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The data",
      "How to approach synthetic data"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/citing-data.html",
    "href": "docs/reproducibility-guidance/howtos/citing-data.html",
    "title": "How to cite open data",
    "section": "",
    "text": "Citing an open dataset for a research project is straightforward if it is archived in an open research repository like Zenodo. In this case it will have the relevant metadata, along with a DOI, to make it easily citeable, even if access to these data is restricted. Unfortunately open datasets do not always reside in a repository like Zenodo. A common place for open datasets in statistics, and in particular price statistics, is as part of an R package on CRAN. Citing these datasets is not difficult—all CRAN packages have a DOI and are easily cited—but, as part of a package, data are more difficult to access.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to cite open data"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/citing-data.html#example-priceindices",
    "href": "docs/reproducibility-guidance/howtos/citing-data.html#example-priceindices",
    "title": "How to cite open data",
    "section": "Example: {PriceIndices}",
    "text": "Example: {PriceIndices}\nThe {PriceIndices} R package comes with several datasets that can be useful for comparing different index-number methods. Citing these data is easy because R packages are highly citeable.\n@Manual{,\n  title = {PriceIndices: Calculating Bilateral and Multilateral Price Indexes},\n  author = {Jacek Białek},\n  year = {2025},\n  doi = {10.32614/CRAN.package.PriceIndices},\n  url = {https://cran.r-project.org/package=PriceIndices},\n  note = {R package version 0.2.5}\n}\nUsing these data is easy enough with R. Simply install the package and the datasets become available from within R.\n\nhead(PriceIndices::coffee)\n\n        time prices quantities prodID retID    description\n1 2017-12-01 123.55         13  32308  2183 instant coffee\n2 2017-12-01  56.08         16  51858  2183 instant coffee\n3 2017-12-01  66.90         11  51859  2183 instant coffee\n4 2017-12-01  66.90         10  51860  2183 instant coffee\n5 2017-12-01  34.30         11  51863  2183 instant coffee\n6 2017-12-01 141.20         10  55880  2183 instant coffee\n\n\nUsing these datasets is less convenient with, say, Python. One option is to simply export them in an interoperable format like Apache Parquet from R.\n\narrow::write_parquet(PriceIndices::coffee, \"coffee.parquet\")\n\nNow it is simple to use these data with Python.\n\nimport pandas as pd\n\npd.read_parquet(\"coffee.parquet\").head()\n\n         time  prices  quantities  prodID  retID     description\n0  2017-12-01  123.55          13   32308   2183  instant coffee\n1  2017-12-01   56.08          16   51858   2183  instant coffee\n2  2017-12-01   66.90          11   51859   2183  instant coffee\n3  2017-12-01   66.90          10   51860   2183  instant coffee\n4  2017-12-01   34.30          11   51863   2183  instant coffee\n\n\nAnother approach, and one that doesn’t use R, is to simply download the {PriceIndices} package from CRAN and uses the {rdata} Python package to read the datasets in Python.\n\ncurl -s https://cran.r-project.org/src/contrib/PriceIndices_0.2.5.tar.gz -o PriceIndices_0.2.5.tar.gz\ntar -vxzf PriceIndices_0.2.5.tar.gz PriceIndices/data\n\nPriceIndices/data/\nPriceIndices/data/dataU.rda\nPriceIndices/data/milk.rda\nPriceIndices/data/coffee.rda\nPriceIndices/data/data_DOWN_UP_SIZED.rda\nPriceIndices/data/dataMARS.rda\nPriceIndices/data/dataAGGR.rda\nPriceIndices/data/sugar.rda\nPriceIndices/data/dataMATCH.rda\nPriceIndices/data/dataCOICOP.rda\n\n\n\nimport rdata\nimport pandas as pd\n\n\ndef date_constructor(obj, attr):\n    return pd.to_datetime(obj, unit=\"D\")\n\nconstructor_dict = {**rdata.conversion.DEFAULT_CLASS_MAP,\n                    \"Date\": date_constructor}\n\ncoffee = rdata.read_rda(\"PriceIndices/data/coffee.rda\",\n                        constructor_dict=constructor_dict)\n\npd.DataFrame(coffee[\"coffee\"]).head()\n\n        time  prices  quantities  prodID  retID     description\n1 2017-12-01  123.55          13   32308   2183  instant coffee\n2 2017-12-01   56.08          16   51858   2183  instant coffee\n3 2017-12-01   66.90          11   51859   2183  instant coffee\n4 2017-12-01   66.90          10   51860   2183  instant coffee\n5 2017-12-01   34.30          11   51863   2183  instant coffee",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to cite open data"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/how-to-cite-digital-objects.html",
    "href": "docs/reproducibility-guidance/howtos/how-to-cite-digital-objects.html",
    "title": "How to reference digital objects you create",
    "section": "",
    "text": "When writing a paper, a structured approach to citing your research compendium is useful as it creates a coherent approach among the discipline.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to reference digital objects you create"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/how-to-cite-digital-objects.html#visual",
    "href": "docs/reproducibility-guidance/howtos/how-to-cite-digital-objects.html#visual",
    "title": "How to reference digital objects you create",
    "section": "Visual",
    "text": "Visual\nBuilding on the visual seen earlier of the 4 objects of the resarch process, Figure 1 shows how each can be cited.\n\n\n\n\n\n\nFigure 1: Citing research objects",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to reference digital objects you create"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/how-to-cite-digital-objects.html#how-to-cite-each-in-your-paper",
    "href": "docs/reproducibility-guidance/howtos/how-to-cite-digital-objects.html#how-to-cite-each-in-your-paper",
    "title": "How to reference digital objects you create",
    "section": "How to cite each in your paper",
    "text": "How to cite each in your paper\nInspired by the FAIR research code approach, we recommend including references to the digital byproducts made available alongside the research in the following manner at the end of your paper.\nAdd two headers just after the conclusion and before the references section:\n\nData\nCite the input data used in the data section of the paper\n….. other sections\n… conclusion of the paper\nData availability\nRererence the output datasets created as part of the research.\nCode availability\nReference the research compendium (the archived version ideally, but the active GitHub link works too if no archive was made).\nReferences\n… rest of the references\n\nUsing proper citation style throughout the paper ensures that the bibliography has the full set of references. The standard structure (while not absolutely necessary as footnotes or other approaches also work) helps ensure that researchers in the discipline follow a standard approach.",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "How to reference digital objects you create"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/publishing-research-compendium.html",
    "href": "docs/reproducibility-guidance/howtos/publishing-research-compendium.html",
    "title": "Publishing your research compendium",
    "section": "",
    "text": "CautionWIP\n\n\n\nThis page is still in the works. Scope to include:\n\nStarting off, publish to GitHub - you can cite a paper in GitHub\nPush the research compendium to a repository that assigns a DOI\n\nZenodo for Python\nCRAN for R\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "Publishig artifacts and citation",
      "Publishing your research compendium"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/howtos/licences.html",
    "href": "docs/reproducibility-guidance/howtos/licences.html",
    "title": "What licences could I use?",
    "section": "",
    "text": "CautionWIP\n\n\n\nThis page is still in the works. Possile topics (see #52 for more info) could include:\n\nLicenses for code (such as by NHS RAP guide)\nData licenses (such as by How to FAIR)\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "How to be reproducible",
      "Guides and how-tos",
      "The research compendium",
      "What licences could I use?"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/open-material.html",
    "href": "docs/reproducibility-guidance/open-material.html",
    "title": "Open material for price statistics",
    "section": "",
    "text": "There are many open-source software packages for official statistics and survey statistics. The two best lists of these packages are:\n\nAwesome list of official statistics software\nCRAN task view for Official Statistics\n\nEurostat also has a similar list that’s worth exploring:\n\neurostat@github\n\nAlthough some of the packages on these lists are for computing index numbers (and are enumerated in the next section), there are several complementary tools for, e.g., sampling, seasonal adjustment, that are nonetheless important for price statistics.\nThe UNECE High-Level Group for the Modernisation of Official Statistics gives a good overview of the role of open-source software in the production official statistics, and can help contextualize the software on these lists.",
    "crumbs": [
      "How to be reproducible",
      "Open material for price statistics"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/open-material.html#software-for-official-statistics",
    "href": "docs/reproducibility-guidance/open-material.html#software-for-official-statistics",
    "title": "Open material for price statistics",
    "section": "",
    "text": "There are many open-source software packages for official statistics and survey statistics. The two best lists of these packages are:\n\nAwesome list of official statistics software\nCRAN task view for Official Statistics\n\nEurostat also has a similar list that’s worth exploring:\n\neurostat@github\n\nAlthough some of the packages on these lists are for computing index numbers (and are enumerated in the next section), there are several complementary tools for, e.g., sampling, seasonal adjustment, that are nonetheless important for price statistics.\nThe UNECE High-Level Group for the Modernisation of Official Statistics gives a good overview of the role of open-source software in the production official statistics, and can help contextualize the software on these lists.",
    "crumbs": [
      "How to be reproducible",
      "Open material for price statistics"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/open-material.html#software-for-price-statistics",
    "href": "docs/reproducibility-guidance/open-material.html#software-for-price-statistics",
    "title": "Open material for price statistics",
    "section": "Software for price statistics",
    "text": "Software for price statistics\n\n\n\n\n\n\nNoteThis list is a work in progress\n\n\n\nPlease let us know about any software packages relevant for price statistics that we missed.\n\n\nMost software for price statistics is implemented in R, with the remainder in Python. The projects listed here may differ in various aspects, such as maturity level, maintenance status, licensing terms, and more. Users are encouraged to assess whether each open-source tool aligns with their specific requirements. Note that this list is for software packages related to price statistics that are broadly available on, e.g., CRAN or PyPI, not data analysis scripts used to derive a price index. Packages enable efficient code reuse across multiple projects and are more likely to be useful to the broader community.\nOpen-source software packages for price statistics are split into those for computing index numbers and those to facilitate accessible official statistics. There are also several software papers related to price statistics.\n\nComputing price indexes\n\nMultilaterals and scanner data\n\nIndexNumR \nPriceIndices \n\n\n\nAggregation\n\npiar \n\n\n\nHousing\n\nhpiR \nrsmatrix \n\n\n\nGeneral purpose\n\nIndexNumberTools \n\n\n\n\nAccessing price indexes\n\nauxdex \ncansim \nhicp \npalewire \n\n\n\nSoftware papers\n\ndff\nIndexNumber: An R Package for Measuring the Evolution of Magnitudes\npiar: Price Index Aggregation in R\n\n\n\nOlder/inactive projects\n\nFEWS \nGEKSdecomp \nIndexNumber \nInflation \nmicEconIndex \nmultilateral \nprecon \nPriceIndexCalc \nproductivity \nTPDdecomp",
    "crumbs": [
      "How to be reproducible",
      "Open material for price statistics"
    ]
  },
  {
    "objectID": "docs/reproducibility-guidance/open-material.html#reproducibility-resources",
    "href": "docs/reproducibility-guidance/open-material.html#reproducibility-resources",
    "title": "Open material for price statistics",
    "section": "Reproducibility resources",
    "text": "Reproducibility resources\nThere are several good resources for how to make data analysis and research projects reproducible. These are not about price statistics per se, but the ideas and tools are broadly applicable to the field. The example of a price-index pipeline shows how these ideas around reproducibility can pair with the software listed above.\n\nGeneral\n\nThe Turing Way\nAwesome Reproducible Research\n\n\n\nReproducible analytical pipelines\n\nRAP Companion\nUdemy course on RAP with R\nReproducible Analytical Pipelines or RAP (NHS) site\nESCAP training on RAP with web scraping application for price statistics\nBuilding reproducible analytical pipelines with R\nProjects with targets",
    "crumbs": [
      "How to be reproducible",
      "Open material for price statistics"
    ]
  }
]