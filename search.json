[
  {
    "objectID": "project-content/project-charter.html",
    "href": "project-content/project-charter.html",
    "title": "Project Charter",
    "section": "",
    "text": "Research in the price statistics discipline is not as reproducible as we feel it should be. Most researchers utilize proprietary datasets (for instance internal datasets owned by their NSOs as part of their official work, or purchased datasets that require considerable financial investment for others to acquire). Research is also done using software available to researchers in a way that is custom to them and the code and detailed processes are typically not made easily available as part of the research project. This consequence is far from the intention of researchers, but is a result of the challenges to do this within the discipline. Specifically, it is not easy to find or access open datasets that can be used for research purposes. Once data is found, the metadata on each dataset will differ, making it challenging to process and use the data, such as for repeatable research processes. Finally, once a researcher has access to the data, it is not clear how code and processing logic should be shared as part of the research project to make the project reproducible. In other words, the process is not Findable, Accessible, Interoperable, or Reusable (or FAIR).\n\n\n\nThe project aims to simplify this situation in the price statistics discipline by tackling the challenges that researchers face. In other words, the project aims to lower the barrier for reproducibility, making it intuitively easy (with some practice) for researchers to work openly. From an open science point of view, making research more routinely reproducible will help accelerate the pace at which consensus is reached on various topics as results can quickly become replicable and even generalizable.\n\n\n\nAs the project tackles the main challenges facing the discipline, it aims to deliver the following aspects:\n\nObjective A: Developing an interim data catalogue tool for several open datasets and publishing several open datasets as a proof of concept. The idea is to start helping researchers know how to reference open datasets for better replicability within the discipline\n\nA.1. Mock up an interim data catalogue on the UNGP GitLab static site that is easy to read through and understand. The catalogue will be made available on its own GitHub repo.\nA.2. Investigate the applicable metadata for publishing/registering a dataset and outline an ingestion/registration process that can be leveraged to onboard datasets to the interim data catalogue.\nA.3. Coordinate the registering of several open datasets that can act as a pilot for the interim data catalogue.\n\nObjective B: Developing a white paper to outline the why, the what, the where, the when (in the research process) and the how. The idea is to create a high level guide for researchers, and could build on the FAIR or reproducibility literature and apply it to our domain. The paper would have several sub components that should be investigated and discussed, such as:\n\nB.1. Investigate processes around data – such as how to reference the data (internal, public, or synthetic), how to incentivize the use of benchmark datasets for specific tasks in the discipline, how to deal with complex use cases (such as confidentiality or privately owned but widely used data).\nB.2. Investigate processes around code and related objects like clear code documentation – such as where to publish code, what should be included in the repository, how to clearly document the process, etc. If applicable, make template repositories or examples available for the community.\nB.3. Investigate administrative topics that are useful for the discipline, such as how to coordinate with the 2 conferences we attend to make sure that we embed and incentivize the use of the processes we would like to develop.\nB.4. Outlining of the white paper/guidance for the discipline on reproducibility\n\n\n\n\n\nTo manage the project, a GitHub projects is used for coordination and transparency."
  },
  {
    "objectID": "project-content/project-charter.html#project-overview",
    "href": "project-content/project-charter.html#project-overview",
    "title": "Project Charter",
    "section": "",
    "text": "Research in the price statistics discipline is not as reproducible as we feel it should be. Most researchers utilize proprietary datasets (for instance internal datasets owned by their NSOs as part of their official work, or purchased datasets that require considerable financial investment for others to acquire). Research is also done using software available to researchers in a way that is custom to them and the code and detailed processes are typically not made easily available as part of the research project. This consequence is far from the intention of researchers, but is a result of the challenges to do this within the discipline. Specifically, it is not easy to find or access open datasets that can be used for research purposes. Once data is found, the metadata on each dataset will differ, making it challenging to process and use the data, such as for repeatable research processes. Finally, once a researcher has access to the data, it is not clear how code and processing logic should be shared as part of the research project to make the project reproducible. In other words, the process is not Findable, Accessible, Interoperable, or Reusable (or FAIR)."
  },
  {
    "objectID": "project-content/project-charter.html#raison-dêtre-of-the-project",
    "href": "project-content/project-charter.html#raison-dêtre-of-the-project",
    "title": "Project Charter",
    "section": "",
    "text": "The project aims to simplify this situation in the price statistics discipline by tackling the challenges that researchers face. In other words, the project aims to lower the barrier for reproducibility, making it intuitively easy (with some practice) for researchers to work openly. From an open science point of view, making research more routinely reproducible will help accelerate the pace at which consensus is reached on various topics as results can quickly become replicable and even generalizable."
  },
  {
    "objectID": "project-content/project-charter.html#expected-outcomes-in-a-little-more-detail",
    "href": "project-content/project-charter.html#expected-outcomes-in-a-little-more-detail",
    "title": "Project Charter",
    "section": "",
    "text": "As the project tackles the main challenges facing the discipline, it aims to deliver the following aspects:\n\nObjective A: Developing an interim data catalogue tool for several open datasets and publishing several open datasets as a proof of concept. The idea is to start helping researchers know how to reference open datasets for better replicability within the discipline\n\nA.1. Mock up an interim data catalogue on the UNGP GitLab static site that is easy to read through and understand. The catalogue will be made available on its own GitHub repo.\nA.2. Investigate the applicable metadata for publishing/registering a dataset and outline an ingestion/registration process that can be leveraged to onboard datasets to the interim data catalogue.\nA.3. Coordinate the registering of several open datasets that can act as a pilot for the interim data catalogue.\n\nObjective B: Developing a white paper to outline the why, the what, the where, the when (in the research process) and the how. The idea is to create a high level guide for researchers, and could build on the FAIR or reproducibility literature and apply it to our domain. The paper would have several sub components that should be investigated and discussed, such as:\n\nB.1. Investigate processes around data – such as how to reference the data (internal, public, or synthetic), how to incentivize the use of benchmark datasets for specific tasks in the discipline, how to deal with complex use cases (such as confidentiality or privately owned but widely used data).\nB.2. Investigate processes around code and related objects like clear code documentation – such as where to publish code, what should be included in the repository, how to clearly document the process, etc. If applicable, make template repositories or examples available for the community.\nB.3. Investigate administrative topics that are useful for the discipline, such as how to coordinate with the 2 conferences we attend to make sure that we embed and incentivize the use of the processes we would like to develop.\nB.4. Outlining of the white paper/guidance for the discipline on reproducibility"
  },
  {
    "objectID": "project-content/project-charter.html#project-management",
    "href": "project-content/project-charter.html#project-management",
    "title": "Project Charter",
    "section": "",
    "text": "To manage the project, a GitHub projects is used for coordination and transparency."
  },
  {
    "objectID": "project-content/other-open-materials.html",
    "href": "project-content/other-open-materials.html",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "The following list is a mini-compenidum of other materials that may be of interest to reseachers in the prices statistics discipline. The list is meant to be live and up to date, hence if you see any inaccuracies, a broken link, or something missing - please feel free to add!\n\n\n\nAwesome list of official statistics software\nReferences in JOSS paper\nSteve’s stars\n\n\n\n\nhicp package (Eurostat) - provides functions to access and work with HICP data (price indices and weights) from Eurostat’s public database\n‘PriceIndices’ package\nIndexNumR package\npiar package\nCRAN task view for official statistics\n\n\n\n\n\nPriceIndexCalc package - a partially working package to calculate price indices.\n\n\n\n\n\ndff - a repo on Dominick’s Finer Foods showcasing the dataset by Jens Mehrhoff\n\n\n\n\n\n\n\n\nThe Turing Way - an open science/collaboration project\nreproducible-project-template - repo template (i.e cookie cutter template)\n\n\n\n\n\nRAP Companion, by Matthew Gregory and Matthew Upson shows much of the original RAP ideas (for R)\nUdemy course on RAP with R\nReproducible Analytical Pipelines or RAP (NHS) site\nESCAP training on RAP with web scraping application for price statistics"
  },
  {
    "objectID": "project-content/other-open-materials.html#packages-applicable-to-price-statistics",
    "href": "project-content/other-open-materials.html#packages-applicable-to-price-statistics",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "Awesome list of official statistics software\nReferences in JOSS paper\nSteve’s stars\n\n\n\n\nhicp package (Eurostat) - provides functions to access and work with HICP data (price indices and weights) from Eurostat’s public database\n‘PriceIndices’ package\nIndexNumR package\npiar package\nCRAN task view for official statistics\n\n\n\n\n\nPriceIndexCalc package - a partially working package to calculate price indices.\n\n\n\n\n\ndff - a repo on Dominick’s Finer Foods showcasing the dataset by Jens Mehrhoff"
  },
  {
    "objectID": "project-content/other-open-materials.html#reproducibility-resources",
    "href": "project-content/other-open-materials.html#reproducibility-resources",
    "title": "List of open materials relevant to price statistics",
    "section": "",
    "text": "The Turing Way - an open science/collaboration project\nreproducible-project-template - repo template (i.e cookie cutter template)\n\n\n\n\n\nRAP Companion, by Matthew Gregory and Matthew Upson shows much of the original RAP ideas (for R)\nUdemy course on RAP with R\nReproducible Analytical Pipelines or RAP (NHS) site\nESCAP training on RAP with web scraping application for price statistics"
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About this project",
    "section": "",
    "text": "This project is led by Workstream 5 of the UN Task Team on Scanner data, part of the UN Committee of Experts on Big Data and Data Science for Official Statistics (UN-CEBD)."
  },
  {
    "objectID": "docs/about.html#what-this-project-aims-to-do",
    "href": "docs/about.html#what-this-project-aims-to-do",
    "title": "About this project",
    "section": "What this project aims to do",
    "text": "What this project aims to do\nResearch in the price statistics discipline is not as reproducible as it could be. Most researchers utilize proprietary datasets and don’t publish the code alongside the research so that a specific methodology or finding can be easily reproduced. The idea of this project is to help in both topics! We aim to:\n\nProvide clear and approachable guidance on how researchers in price statistics can make their projects reproducible (including by learning new skills, by understanding how to publish and cite code, and many other topics).\nSupport cataloging open datasets that can be used as benchmarks to use for research purposes. The idea is to have all findings trialed and demonstrated on open benchmark datasets.\n\nThe project also builds on great efforts of early promoters of open science in the discipline!1 We are aiming to continue to build momentum and make it easier for researchers to work openly!"
  },
  {
    "objectID": "docs/about.html#what-this-project-does-not-aim-to-do",
    "href": "docs/about.html#what-this-project-does-not-aim-to-do",
    "title": "About this project",
    "section": "What this project does not aim to do",
    "text": "What this project does not aim to do\n\n\n\n\n\n\nThis section is still a WIP\n\n\n\nNot all team members are listed, we will update this section soon.\n\n\nAs there is a lot of great guidance on the topic already, including The Turing Way, Reproducible Analytical Pipelines or RAP, and many more resources—the idea is to distill key information for the price statistics community, not to create a new standard."
  },
  {
    "objectID": "docs/about.html#serge-goussev",
    "href": "docs/about.html#serge-goussev",
    "title": "About this project",
    "section": "Serge Goussev",
    "text": "Serge Goussev\n\nRole: Workstream lead (2024 - current)\nGitHub id: sergegoussev\nEmail: serge.goussev@statcan.gc.ca"
  },
  {
    "objectID": "docs/about.html#steve-martin",
    "href": "docs/about.html#steve-martin",
    "title": "About this project",
    "section": "Steve Martin",
    "text": "Steve Martin\n\nRole: Contributor (2024 - current)\nGitHub id: marberts"
  },
  {
    "objectID": "docs/about.html#claude-lamboray",
    "href": "docs/about.html#claude-lamboray",
    "title": "About this project",
    "section": "Claude Lamboray",
    "text": "Claude Lamboray\n\nRole: Contributor (2024 - current)"
  },
  {
    "objectID": "docs/about.html#christophe-bontemps",
    "href": "docs/about.html#christophe-bontemps",
    "title": "About this project",
    "section": "Christophe Bontemps",
    "text": "Christophe Bontemps\n\nRole: Contributor (2024 - current)"
  },
  {
    "objectID": "docs/about.html#tanya-flower",
    "href": "docs/about.html#tanya-flower",
    "title": "About this project",
    "section": "Tanya Flower",
    "text": "Tanya Flower\n\nRole: Contributor (2024 - current)"
  },
  {
    "objectID": "docs/about.html#ben-hillman",
    "href": "docs/about.html#ben-hillman",
    "title": "About this project",
    "section": "Ben Hillman",
    "text": "Ben Hillman\n\nRole: Contributor (2024 - current)"
  },
  {
    "objectID": "docs/about.html#jens-mehrhoff",
    "href": "docs/about.html#jens-mehrhoff",
    "title": "About this project",
    "section": "Jens Mehrhoff",
    "text": "Jens Mehrhoff\n\nRole: Contributor (2024 - current)\nEmail: jens.mehrhoff@bundesbank.de"
  },
  {
    "objectID": "docs/about.html#footnotes",
    "href": "docs/about.html#footnotes",
    "title": "About this project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotable early efforts include but are not limited to Frances Krsinich publishing the FEWS package with open data in 2018, Jens Mehrhoff showcasing the open Dominick’s Finer Foods scanner dataset in 2019, Erwin Dielert and Chihiro Shimizu opening a dataset of japanese laptop sales, or Jacek Białek including datasets with the PriceIndices R package.↩︎"
  },
  {
    "objectID": "docs/faq.html",
    "href": "docs/faq.html",
    "title": "Frequently asked questions",
    "section": "",
    "text": "Help us fill this page!\n\n\n\nHave an idea for a common topic to add to this FAQ? Give us a shout! Submit an issue to the project and tag the the issue using the “question” label.\n\n\n\nHow do I register a dataset to the catalogue (or recommend a dataset be registered)?\nWe provide guidance on how you can do both here.\n\n\nWhat training can I take to practice technical skills to create and publish a research compendium for my project?\nWe are exploring more targeted support, however for now we recommend that practice is the best training. Try it out! The following guides are great to review and get started:\n\nThe Turing Way\nThe RAP Community of Practice (hosted by the NHS in the UK)\n\n\n\nHow do I select the appropriate tools (GitHub) or data repository?\nFirstly, you should follow the guidance of your organization. They will guide you what you can and cannot do. For instance they may have processes to follow to publish to GitHub or a specific data repository/process to use for data you want to publish.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html",
    "href": "docs/reproducibility-guidance/intro.html",
    "title": "How to approach reproducibility",
    "section": "",
    "text": "For robust scientific progress, new methods or approaches should be confirmed independently before they are widely adopted. The goal of appropriately structuring and sharing research objects in a transparent fashion is to simplify this peer review process.1 The primary way this is achieved by creating and publicly publishing a research compendium along with the paper. A research compendium is a collection of digital parts of the research project that supports reuse, including data, code, protocols, metadata, etc.2"
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#what-is-reproducibility",
    "href": "docs/reproducibility-guidance/intro.html#what-is-reproducibility",
    "title": "How to approach reproducibility",
    "section": "",
    "text": "For robust scientific progress, new methods or approaches should be confirmed independently before they are widely adopted. The goal of appropriately structuring and sharing research objects in a transparent fashion is to simplify this peer review process.1 The primary way this is achieved by creating and publicly publishing a research compendium along with the paper. A research compendium is a collection of digital parts of the research project that supports reuse, including data, code, protocols, metadata, etc.2"
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#why-does-it-help",
    "href": "docs/reproducibility-guidance/intro.html#why-does-it-help",
    "title": "How to approach reproducibility",
    "section": "Why does it help?",
    "text": "Why does it help?\nThe main principle of the research compendium is to provide all the information about the project publicly and structured in a clear and logical way to ensure that its use is straightforward. This helps researchers themselves as they can easily jump back to a previous project, simplify the task for reviewers or those who want to extend the research, as well as those simply looking to learn. If done properly, the research compendium will help:3\n\nImprove the transparency, reliability and reproducibility of research.\nSimplify peer review.\nFacilitate data and code sharing.\nAllow easy extension of the research.\nEnable learners to understand the research.\nMake it much easier to transition a new method to production.4\nIncrease research visibility and citations."
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#how-to-structure-the-research-compendium",
    "href": "docs/reproducibility-guidance/intro.html#how-to-structure-the-research-compendium",
    "title": "How to approach reproducibility",
    "section": "How to structure the research compendium",
    "text": "How to structure the research compendium\n\n\n\n\n\n\nConsider this as interim guidance at this stage\n\n\n\nThe guidance on research compendium is at an interim phase. The project team will flesh this out to provide more clarity aligned with the needs of the discipline and how researchers can ramp up (as it’s not an all or nothing task).\n\n\n\nOverview of the structure\nIn a nutshell, a compendium includes all research objects necessary to reproduce the research. In a technical sense, these are often git repositories (in say GitHub) that include a structure similar to the one in Figure 1 below.\n\n\n\n\n\n\nFigure 1: Exemplar price index pipeline.\n\n\n\n\n\nA little about each aspect\nA data folder that outlines where to store the raw dataset used for the research project. Ideally the researcher uses an open dataset (which will make the whole process reproducible), but they may also use a proprietary dataset.\n\n\n\n\n\n\nDon’t version control the main input dataset\n\n\n\nThe dataset that acts as the main input dataset to the research should not be version controlled. The folder is created in order to ensure that when a local copy of the compendium is used by researchers, they know where to put the data to ensure that the code will replicate the results in full.\nTechnically, this means making sure that the .gitignore skips this data file\n\n\nA folder for functions (or other code) that helps process your data into the relevant outputs. This could include the code to clean and prepare the raw dataset for research purposes, the code to create the processing and research experiments, as well as notebooks where the data is explored and various aspects that feed the research paper are generated.\nA folder for the output data. This data can be versioned (if it is not sensitive) with the repository and allows researchers to replicate the process. Note, if the output data can be used for research in its own right, it may be appropriate to register this dataset on a public repository (such as Zenodo) that mints a DOI.\nA folder for documentation to that explain key aspects of the research. This folder stores project documentation or the project design, however code should also be documented well.\nA license. This will tell users how they can use the code.\nA .gitignore file. There are some files and folders that should not be version controlled. Notable example is datasets\nA file to recreate the environment on which the code will run identically. A shift in one package version to another may change the output, hence its key to track exactly how to replicate the same environment and get the same result.\nFinally, a README to introduce the project. This will be the landing place when someone navigates to the repository, hence it should describe the project at a glance."
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#how-to-share-a-research-compendium",
    "href": "docs/reproducibility-guidance/intro.html#how-to-share-a-research-compendium",
    "title": "How to approach reproducibility",
    "section": "How to share a research compendium?",
    "text": "How to share a research compendium?\nVersion controlling the research compendium is key.5 Once version controlled, it is best to push it to a hosting service like GitHub. Working with something like GitHub makes it easy for researchers to ensure their projects are well structured and robust."
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#notable-example",
    "href": "docs/reproducibility-guidance/intro.html#notable-example",
    "title": "How to approach reproducibility",
    "section": "Notable example",
    "text": "Notable example\nTo showcase an exemplar for price statistics, we created a mock-up price index pipeline that researchers can explore."
  },
  {
    "objectID": "docs/reproducibility-guidance/intro.html#footnotes",
    "href": "docs/reproducibility-guidance/intro.html#footnotes",
    "title": "How to approach reproducibility",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA good overview of reproducible research is done by the Turing Way. The book also covers open research and many other topics.↩︎\nSee Research Compendia in The Turing Way for more detail on this concept.↩︎\nSee a more in depth overview of the benefits of reproducible research, as well as common barriers.↩︎\nResearch compendia are conceptually quite similar to Reproducible Analytical Pipelines (RAPs), although the latter focuses more on production processes. Hence if the research is easy to reproduce by adopting a compendium structure and making everything easily reproducible, operationalizing of a new method could be dramatically simplified. For more on RAPs in price statistics, see a RAP class recently done by ESCAP, as well as a good paper by Price and Marques (2023) showcasing RAP for production processes.↩︎\nSee overview and explanations of version control in The Turing Way for more info.↩︎"
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-data.html",
    "href": "docs/reproducibility-guidance/citing-data.html",
    "title": "How to cite open data",
    "section": "",
    "text": "Citing an open dataset for a research project is straightforward if it is archived in an open research repository like Zenodo. In this case it will have the relevant metadata, along with a DOI, to make it easily citeable, even if access to these data is restricted. Unfortunately open datasets do not always reside in a repository like Zenodo. A common place for open datasets in statistics, and in particular price statistics, is as part of an R package on CRAN. Citing these datasets is not difficult—all CRAN packages have a DOI and are easily cited—but, as part of a package, data are more difficult to access."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-data.html#example-priceindices",
    "href": "docs/reproducibility-guidance/citing-data.html#example-priceindices",
    "title": "How to cite open data",
    "section": "Example: {PriceIndices}",
    "text": "Example: {PriceIndices}\nThe {PriceIndices} R package comes with several datasets that can be useful for comparing different index-number methods. Citing these data is easy because R packages are highly citeable.\n@Manual{,\n  title = {PriceIndices: Calculating Bilateral and Multilateral Price Indexes},\n  author = {Jacek Białek},\n  year = {2025},\n  doi = {10.32614/CRAN.package.PriceIndices},\n  url = {https://cran.r-project.org/package=PriceIndices},\n  note = {R package version 0.2.3}\n}\nUsing these data is easy enough with R. Simply install the package and the datasets become available from within R.\n# install.packages(\"PriceIndices\")\nhead(PriceIndices::coffee)\nUsing these datasets is less convenient with, say, Python. One option is to simply export them in an interoperable format like Apache Parquet from R.\narrow::write_parquet(PriceIndices::coffee, \"coffee.parquet\")\nNow it is simple to use these data with Python.\nimport pandas as pd\n\npd.read_parquet(\"coffee.parquet\").head()\nAnother approach, and one that doesn’t use R, is to simply download the {PriceIndices} package from CRAN and use the {rdata} Python package to read the datasets in Python.\ncurl -s https://cran.r-project.org/src/contrib/PriceIndices_0.2.3.tar.gz -o PriceIndices_0.2.3.tar.gz\ntar -vxzf PriceIndices_0.2.3.tar.gz PriceIndices/data\nimport rdata\nimport pandas as pd\n\n\ndef date_constructor(obj, attr):\n    return pd.to_datetime(obj, unit=\"D\")\n\nconstructor_dict = {**rdata.conversion.DEFAULT_CLASS_MAP,\n                    \"Date\": date_constructor}\n\ncoffee = rdata.read_rda(\"PriceIndices/data/coffee.rda\",\n                        constructor_dict=constructor_dict)\n\npd.DataFrame(coffee[\"coffee\"]).head()"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "When contributing, post comments and discussion changes you wish to make via Issues.\nFeel free to propose changes by creating Pull Requests. If you don’t have write access, editing a file will create a Fork of this project for you to save your proposed changes to. Submitting a change to a file will write it to a new Branch in your Fork, so you can send a Pull Request."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-contribute",
    "href": "CONTRIBUTING.html#how-to-contribute",
    "title": "Contributing",
    "section": "",
    "text": "When contributing, post comments and discussion changes you wish to make via Issues.\nFeel free to propose changes by creating Pull Requests. If you don’t have write access, editing a file will create a Fork of this project for you to save your proposed changes to. Submitting a change to a file will write it to a new Branch in your Fork, so you can send a Pull Request."
  },
  {
    "objectID": "docs/datasets-guidance/catalogue.html",
    "href": "docs/datasets-guidance/catalogue.html",
    "title": "Catalogue of open data in price statistics",
    "section": "",
    "text": "Researchers often struggle to use open data. They may find it hard to find open datasets for their research projects. If they find open datasets, they may struggle to easily understand them in order to judge that the datasets easily work for their research. Finally they may find it challenging to know how to work with or even how to cite the dataset.\nTo help find, assess, and understand how to access open datasets, this project has developed a basic discipline specific catalogue."
  },
  {
    "objectID": "docs/datasets-guidance/catalogue.html#how-the-price-statistics-open-data-catalogue-works-in-a-nutshell",
    "href": "docs/datasets-guidance/catalogue.html#how-the-price-statistics-open-data-catalogue-works-in-a-nutshell",
    "title": "Catalogue of open data in price statistics",
    "section": "How the Price Statistics Open Data Catalogue works in a nutshell",
    "text": "How the Price Statistics Open Data Catalogue works in a nutshell\nThe idea is simple. The catalogue lists open datasets relevant to the discipline and is searchable according to standard data types (such as scanner data). It also provides basic information about each dataset that will allow researchers to assess its relevance and know how to access it. Visually this is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Basic idea of how we see the data catalogue working."
  },
  {
    "objectID": "docs/datasets-guidance/catalogue.html#where-is-the-data-catalogue",
    "href": "docs/datasets-guidance/catalogue.html#where-is-the-data-catalogue",
    "title": "Catalogue of open data in price statistics",
    "section": "Where is the data catalogue?",
    "text": "Where is the data catalogue?\nThe data catalogue can be found here. It is basically a simple static site hosted on GitHub."
  },
  {
    "objectID": "docs/datasets-guidance/catalogue.html#how-to-register-an-open-dataset-on-the-catalogue",
    "href": "docs/datasets-guidance/catalogue.html#how-to-register-an-open-dataset-on-the-catalogue",
    "title": "Catalogue of open data in price statistics",
    "section": "How to register an open dataset on the catalogue?",
    "text": "How to register an open dataset on the catalogue?\nThe CONTRIBUTING guidance in the catalogue summarizes it all. Have a look and help register a dataset or recommend one be registered!"
  },
  {
    "objectID": "docs/datasets-guidance/catalogue.html#what-does-the-catalogue-not-do",
    "href": "docs/datasets-guidance/catalogue.html#what-does-the-catalogue-not-do",
    "title": "Catalogue of open data in price statistics",
    "section": "What does the catalogue not do?",
    "text": "What does the catalogue not do?\nAs the catalogue does not store the dataset itself but simply describes it in detail. In other words, this catalogue is not a data repository.\n\n\n\n\n\n\nThis is an interim catalogue only!\n\n\n\nThis will very likely not be the long-term stable data catalogue used in the discpline. The idea, however, it to start with this interim (and very simple open-source) catalogue, while the project investigates a more viable longer term solution."
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-cite-digital-objects.html",
    "href": "docs/reproducibility-guidance/how-to-cite-digital-objects.html",
    "title": "How to reference digital objects in your paper",
    "section": "",
    "text": "When writing a paper, researchers have several options for how to cite their research compendium, open data, as well as code they used."
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-cite-digital-objects.html#how-to-reference-datasets-you-use-or-other-peoples-code",
    "href": "docs/reproducibility-guidance/how-to-cite-digital-objects.html#how-to-reference-datasets-you-use-or-other-peoples-code",
    "title": "How to reference digital objects in your paper",
    "section": "How to reference datasets you use or other people’s code?",
    "text": "How to reference datasets you use or other people’s code?\nThe best place to reference others’ code or open datasets is in the bibliography. Ideally these should have a Digitial Object Identifier (or DOI) so they are unique and can be easily found. There may be harder cases, which are detailed out in more detail in separate articles:\n\nHow to cite data.\nHow to cite others’ code."
  },
  {
    "objectID": "docs/reproducibility-guidance/how-to-cite-digital-objects.html#how-to-reference-data-or-code-created-as-part-of-the-research",
    "href": "docs/reproducibility-guidance/how-to-cite-digital-objects.html#how-to-reference-data-or-code-created-as-part-of-the-research",
    "title": "How to reference digital objects in your paper",
    "section": "How to reference data or code created as part of the research?",
    "text": "How to reference data or code created as part of the research?\nAs there will be digital byproducts that are made available alongside the research (the research compendium and any new datasets created), inspired by the FAIR research code approach, the project has guidance on how to reference that in the paper so that others can easily find it. We recommend adding two headers just after the conclusion and before the references section\n\n… conclusion of the paper\nData availability\nSummarize where datasets created as part of the paper are published. This could be on a data repository like Zenodo for instance.\nCode availability\nAdd link to the GitHub repository where the research compendium is published.\nReferences\n… rest of the references"
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-code.html",
    "href": "docs/reproducibility-guidance/citing-code.html",
    "title": "How to cite code",
    "section": "",
    "text": "Citing software is historically not something that researchers have done. Code can often be hard to cite and the software used to derive the results of a paper was usually seen as a “detail” that is not worth mentioning in the body of paper. (Perhaps to avoid too many questions about how, exactly, the results in the paper were derived.) The last decade has seen a move towards more reproducible research in economics—mainstream journals require code and usually data for quantitative papers—and the proliferation of open-source research software has made it easier to reliably cite the software used for research.\nCiting code is particularly important in the area of price statistics to facilitate reproducible research. There are a great many price index methods, often with fiddly variations, used by different researchers and it is important to know the exact implementation used to generate a price index in order to reproduce the construction of that index. Research for price statistics is also usually done by government agencies; transparency about the code to generate the results that inform the methods used by government agencies is important to maintain the trust in official statistics.\nFor further reading, the Turing Way provides a good overview for citing code in research and represents the current state of the world of open-source research software."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-code.html#citing-research-software",
    "href": "docs/reproducibility-guidance/citing-code.html#citing-research-software",
    "title": "How to cite code",
    "section": "Citing research software",
    "text": "Citing research software\nUnlike academic papers, there is no one way to cite a piece of software. How to cite a piece of software usually depends on how easy the author makes it. For example, R packages are easy to cite within R using the citation() function.\n\ncitation(\"IndexNumR\")\n\nTo cite package 'IndexNumR' in publications use:\n\n  White G (2023). _IndexNumR: Index Number Calculation_. R package\n  version 0.6.0, &lt;https://github.com/grahamjwhite/IndexNumR&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {IndexNumR: Index Number Calculation},\n    author = {Graham White},\n    year = {2023},\n    note = {R package version 0.6.0},\n    url = {https://github.com/grahamjwhite/IndexNumR},\n  }\n\nATTENTION: This citation information has been auto-generated from the\npackage DESCRIPTION file and may need manual editing, see\n'help(\"citation\")'.\n\n\nModern R packages tend to have their own website with this information on display and all CRAN packages get a DOI to facilitate referencing the use of R packages. Things are less standardized in the Python ecosystem, but the same ideas apply to make projects citeable (e.g., pandas).\nAnother way to cite a piece of research software is to the cite the paper introducing this software, usually in a journal like the Journal of Open Source Software, the Journal of Statistical Software, or the R Journal.\n@article{RJ-2021-038,\n  author = {Saavedra-Nieves, Alejandro and Saavedra-Nieves, Paula},\n  title = {IndexNumber: An R Package for Measuring the Evolution of Magnitudes},\n  journal = {The R Journal},\n  year = {2021},\n  note = {https://rjournal.github.io/},\n  volume = {13},\n  issue = {1},\n  issn = {2073-4859},\n  pages = {253-275}\n}\nFinally, for code that is not in a mainstream repository like CRAN, or does not have a website with citation information, citation information can sometimes be found in the source code. Github helps to find the citation information for a package and displays a button to cite the repository."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-code.html#making-software-citable",
    "href": "docs/reproducibility-guidance/citing-code.html#making-software-citable",
    "title": "How to cite code",
    "section": "Making software citable",
    "text": "Making software citable\nGiven the variety of ways to cite research software, the key to making it citeable is making it easy to generate a reference for that software. For R packages this happens automatically if the package is available on CRAN; for Python packages (and R packages not on CRAN), a service like Zenodo can be used to get a DOI to facilitate referencing the software. Although consumers of software tend not to get it directly from a source-code repository, citation metadata can be added to github repositories to make them more citeable."
  },
  {
    "objectID": "docs/reproducibility-guidance/citing-code.html#example",
    "href": "docs/reproducibility-guidance/citing-code.html#example",
    "title": "How to cite code",
    "section": "Example",
    "text": "Example\nThe {piar} R package is an example of a piece of software for making price indexes that is highly citeable.\n\ncitation(\"piar\")\n\nTo cite package 'piar' in publications use:\n\n  Martin S (2024). \"piar: Price Index Aggregation R.\" _Journal of Open\n  Source Software_, *9*(101), 6781. doi:10.21105/joss.06781\n  &lt;https://doi.org/10.21105/joss.06781&gt;.\n\n  Martin S (2024). _piar: Price Index Aggregation_.\n  doi:10.5281/zenodo.10110046\n  &lt;https://doi.org/10.5281/zenodo.10110046&gt;, R package version 0.8.1,\n  &lt;https://cran.r-project.org/package=piar&gt;.\n\nTo see these entries in BibTeX format, use 'print(&lt;citation&gt;,\nbibtex=TRUE)', 'toBibtex(.)', or set\n'options(citation.bibtex.max=999)'.\n\n\nThis information is displayed on the project website and CRAN. The citation information is contained in the source code, and consequently displayed by github, and the readme for the project is adorned with badges giving citation information. The goal is to have citation information available at each entry point at which a prospective user may first engage with {piar} and to have it be easy to add to a reference list."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "",
    "text": "Welcome to the project aiming to help researchers in the field of price statistics make their projects more reproducible and help push open science in the discipline."
  },
  {
    "objectID": "docs/index.html#making-guidance-on-reproducibility-available",
    "href": "docs/index.html#making-guidance-on-reproducibility-available",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Making guidance on reproducibility available",
    "text": "Making guidance on reproducibility available\nThe project thus helps make:\n\nRecommendations on how to approach reproducibility and structure your research compendium.\nRecommendations on structured ways to reference digital objects.\nRecommendations on how to cite others’ code and how to cite open datasets in your research."
  },
  {
    "objectID": "docs/index.html#cataloguing-all-open-datasets-applicable-to-the-discipline",
    "href": "docs/index.html#cataloguing-all-open-datasets-applicable-to-the-discipline",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Cataloguing all open datasets applicable to the discipline",
    "text": "Cataloguing all open datasets applicable to the discipline\nThe project also makes resources available, such as:\n\nThe Price Statistics Open Data Catalogue.\nGuidance on how the Price Statistics Open Data Catalogue works.\nGuidance on how to register open dataset to the Price Statistics Data Catalogue."
  },
  {
    "objectID": "docs/index.html#footnotes",
    "href": "docs/index.html#footnotes",
    "title": "Welcome to the Price Statistics Reproducibility Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Definitions in The The Turing Way for more details.↩︎\nFor instance see sessions by UN ESCAP, UN SIAP, or the University of Luxembourg as example classes.↩︎\nSee Price and Marques (2023) for an overview of RAP to price statistics.↩︎"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "Code of Conduct\nWe are a community based on openness, as well as friendly and didactic discussions.\nWe aspire to treat everybody equally, and value their contributions.\nDecisions are made based on technical merit and consensus.\nCode is not the only way to help the project. Reviewing pull requests, answering questions to help others on mailing lists or issues, organizing and teaching tutorials, working on the website, improving the documentation, are all priceless contributions.\nWe abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/\n\n\n\n\n Back to top"
  },
  {
    "objectID": "project-content/meeting-minutes.html",
    "href": "project-content/meeting-minutes.html",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "This document summarizes the meetings of the workstream\n\n\n\n\n\nIntroductions\nDiscussions on problem statement and possible solutions\nIdeas on how to go forward\nWrap up and immediate next steps\n\n\n\n\n\nGroup met with context that reproducibility is poor in price statistics research and we should improve it. The group then decided how to tackle this problem and how to properly scope the objectives to make things achievable. Main challenges currently faced in the discipline included:\n\nReserach is becoming increasingly empirical, hence we need processes to work with data and code to make the research more easily reproducible\nThere are no agreed upon benchmark datasets per se in the discipline to test methods on\nThe discipline will not own most of the datasets as most research is done on either confidential internal datasets owned by NSOs or on proprietary (purchased datasets). The open datasets that exist are not organized or cohesively documented/available.\nThe reason why reproducibility is important is not as inherent/widely communicated as it could be, hence any attempts to solve the technical and processes aspects needs to include this aspect in the communication.\n\nThe group touched on goals to solve these aspects and on processes that can be set up to incentivize reproducibility, including by lowering the complexity and creating an easy ‘on ramp’ to making projects more reproducible (including by cataloging open datasets and showcasing how code can be made reproducible), coordinating with the two bi-annual confernece to recommend reproducibility be part of the paper submission process, embedding metadata standards into the data availbile to make it easier and more standardized when various benchmark datasets are used to evaluate a specific method, etc.\nThe data catalogue for open datasets was seen as a major deliverable to the discipline, and one that needed to be phased. In other words we can set up a ‘proof of concept’ or interim catalogue in a simple way to demonstrate the use case, and later transition to a fuller and more comprehensive catalogue with more resources and infrastructure, potentially hosted by the UN Global Platform. Showcasing the interim solution and broadening the adoption to beyond just price statistics would help make this business case.\nThe outcome of the discussion resolved to target two main deliverables: developing the proof of concept data catalogue, and writing guidance (for instance in the form of a white paper) on how to make projects more reproducible. This target scope was later summarized through our project charter.\n\n\n\n\n\n\n\n\nIntro and discussion on timelines and scope. Confirm cadence of meetings\nQuick discussion on how to track PM activity. We could use something like GitHub, the UN wiki, or some other option\nDiscussion on Objective A. We have two options (.Stat suite and data contract catalogue) for interim data catalogue.\nDiscussion on Objective B. Can we adopt any best practices from the Turning Way and their template repo for price statistics?\nRoundtable\n\n\n\n\n\nGroup discussed objectives for the 2025 CPI Expert Group meeting. It was decided to focus on an interim data catalogue and provide interim guidance at the conference, with the fuller guidance to be developed over the next year.\nGitHub projects was agreed upon as the structure for PM activity\nThe draft data catalogue idea was given the green light to flush out further as our likely implementation of the interim version. Metadata strandards should be implemented but it would be hard to use a platform where we don’t own the dataset (such as dominik’s data).\nGuidelines for how to develop reproducible research through git was seen as a good way, with the idea that the guidance we would produce would (a) provide the target state to aim for and (b) summarize maturity levels (similar to RAP maturity levels) that showcase how researchers can start easily and progress over time.\n\n\n\n\n\n\n\n\nDiscussion on the proposed project charter for the project (since merged into main)\nUpdate on the 2 repositories for the project (project repo and interim data catalogue repo) and the mocked up project management approach\nRoundtable\n\n\n\n\n\nTeam discussed scope for 2025 CPI EG presentation. The project plan outlined two milestones, one on data catalogue and one on guidance will be the target. The project structure and use of the GitHub project was summarized.\nThe use of the two repos was discussed. The reproducibility-project repo will host the guidance we develop in whatever format we decide (ideas shared including using quarto to write reproducible papers, or a quarto static site if we will more tend towards creating guidance). price-stats-data-catalogue will host the interim open data catalogue.\nScope of the data contract was firmed up - we could aim to catalogue input datasets that are used to create some experimental indices and version the output datasets (that may be price indices or other artifacts) as part of the repository on github (such as by saving them in data folder and formatting them in tidy data format).\nUse of tools like Zenodo was discussed and will be investigated to mint DOIs - ticket #3\n\n\n\n\n\n\n\n\nDiscussion on format for meeting minutes and how to review/approve the minutes each meeting.\nHow to track materials related to reproducibility but are just references to others’ material (not the overall guidance we will provide).\nWhat format is seen as the best way to deliver guidance on reproducibility? It is best to decide an applicable format and stick with it. Options include writing a paper, using a static site, or using the wiki\n\n\n\n\n\nThe team agreed on keeping notes in this meeting-minutes.md file. The team also agreed to the process:\n\nThe note taker would summarize the meeting and would draft a branch and prepare a Pull Request for the team to review the meeting minutes at the next meeting.\nAt the start of the next meeting, the team would review the PR, would commit any changes/fixes needed. and squash and commit the PR into the main branch to approve the minutes.\n\nThe team agreed to track other materials in other-open-materials.md for now\nThe team discussed the means of how guidance will be provided at the end of the day as a lot of material could be included based on the goals in Objective B of our project. Some possible options:\n\na paper (similar to say the FAIR paper on software to be presented at say 2026 Ottawa Group conference) as the main document for guidance and other materials as supporting (like the catalogue). This could use the quarto manuscript format for example.\na static site (again say a quarto one like this one) as the main means of sharing guidance, but also a short paper for the 2026 OG conference as an offline guide\na set of wiki pages similar to other content made by the Task Team\n\nThe team agreed that a static site (the quarto option) is likely the most appropriate as the site can be expanded and maintained as appropriate, a presentation with a link to the site could be provided at conferences.\nThe team discussed planning for the next 1.5 months. The project roadmap outlines the target timelines. As several issues remain unassigned, the team are encouraged to sign up based on bandwidth.\nRoundtable discussion included:\n\nUpdate on Zenodo - which is an option for uploading and citing data (detail in issue 3). There isn’t a process yet identified for datasets that are not owned by the community and where the owner does not upload it to a repository that mints a DOI.\nUpdate on citing data in PriceIndices R package as it has a DOI. Ability to extract the data without installing the package has to be confirmed.\n\n\n\n\n\n\n\n\n\nReview mock up quarto site for the project, as well as contributing and code of conduct\nReview ongoing work prior to the 2025 CPI Expert Group\nRoundtable\n\n\n\n\n\nTeam discussed the mock up site structure with a home and two main sections, one on how to make your projects reproducible by publishing your code and on using open data.\nThe team also discussed the basic contributing guide and code of conduct.\nThe deliverables to be presented at the 2025 CPI Expert Group were discussed as per the view in the project roadmap.\nAn approach on how to import data from a package was discussed. For instance, how should researchers use data from an R package (such as the PriceIndices package, which has datasets we would like to make list in the interim catalogue) and they wanted to import the data and use it in Python? The team discussed on a phased approach: guidance on how to download the dataset and use it in python from the R ecosystem will be proposed; a longer term approach could be to work with dataset owners to get them to publish it on a repository like Zenodo.\nSupport by the UN Global Platform team for the project was also discussed\n\n\n\n\n\n\n\n\nReview of skeleton data catalogue\nReview of the changes to the project site, specifically how to cite code and how to cite data sections\nReview roadmap and discussion of topics left to finish in this phase of the project\n\n\n\n\n\nThe team discussed the skeleton of the proof of concecept data catalogue. The technical process to register new datasets is basically to (1) draft a new yaml file in the datasets/ folder using the datacontrac.cli specifications, and then (2) when the PR is accepted (after releveant review) and merged with the main branch, the runner will rerender the catalogue and the dataset will show up.\nThe team discussed next steps. The dataset in #6 is still the third we’d want for presentation at CPI EG.\nThere is a need to differentiate open versus proprietary but popular datasets. Open datasets will be the focus for now with potential for expansion after the conference.\nThe team discussed how to cite datasets and how to cite code topics, and based on the example by the recent Baker et al (2022) FAIR principles for software paper, we decided to go with a nuanced recomemndation for now:\n\nif data or code that a research uses exists should be included in the bibliography\ndata or code that is created as part of the paper should be (ideally published to something that mints a DOI) but the links to the dataset or code are included at the end of the paper under “Data availability” and “Code availability” headers.\nThe idea of topics to discuss after the confernece was also brought up - the process of creating synthetic datasets.\n\nTo support researchers to structure their code, the team also discussed and endoresed recommendign a template RAP."
  },
  {
    "objectID": "project-content/meeting-minutes.html#kick-off",
    "href": "project-content/meeting-minutes.html#kick-off",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Introductions\nDiscussions on problem statement and possible solutions\nIdeas on how to go forward\nWrap up and immediate next steps\n\n\n\n\n\nGroup met with context that reproducibility is poor in price statistics research and we should improve it. The group then decided how to tackle this problem and how to properly scope the objectives to make things achievable. Main challenges currently faced in the discipline included:\n\nReserach is becoming increasingly empirical, hence we need processes to work with data and code to make the research more easily reproducible\nThere are no agreed upon benchmark datasets per se in the discipline to test methods on\nThe discipline will not own most of the datasets as most research is done on either confidential internal datasets owned by NSOs or on proprietary (purchased datasets). The open datasets that exist are not organized or cohesively documented/available.\nThe reason why reproducibility is important is not as inherent/widely communicated as it could be, hence any attempts to solve the technical and processes aspects needs to include this aspect in the communication.\n\nThe group touched on goals to solve these aspects and on processes that can be set up to incentivize reproducibility, including by lowering the complexity and creating an easy ‘on ramp’ to making projects more reproducible (including by cataloging open datasets and showcasing how code can be made reproducible), coordinating with the two bi-annual confernece to recommend reproducibility be part of the paper submission process, embedding metadata standards into the data availbile to make it easier and more standardized when various benchmark datasets are used to evaluate a specific method, etc.\nThe data catalogue for open datasets was seen as a major deliverable to the discipline, and one that needed to be phased. In other words we can set up a ‘proof of concept’ or interim catalogue in a simple way to demonstrate the use case, and later transition to a fuller and more comprehensive catalogue with more resources and infrastructure, potentially hosted by the UN Global Platform. Showcasing the interim solution and broadening the adoption to beyond just price statistics would help make this business case.\nThe outcome of the discussion resolved to target two main deliverables: developing the proof of concept data catalogue, and writing guidance (for instance in the form of a white paper) on how to make projects more reproducible. This target scope was later summarized through our project charter."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section",
    "href": "project-content/meeting-minutes.html#section",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Intro and discussion on timelines and scope. Confirm cadence of meetings\nQuick discussion on how to track PM activity. We could use something like GitHub, the UN wiki, or some other option\nDiscussion on Objective A. We have two options (.Stat suite and data contract catalogue) for interim data catalogue.\nDiscussion on Objective B. Can we adopt any best practices from the Turning Way and their template repo for price statistics?\nRoundtable\n\n\n\n\n\nGroup discussed objectives for the 2025 CPI Expert Group meeting. It was decided to focus on an interim data catalogue and provide interim guidance at the conference, with the fuller guidance to be developed over the next year.\nGitHub projects was agreed upon as the structure for PM activity\nThe draft data catalogue idea was given the green light to flush out further as our likely implementation of the interim version. Metadata strandards should be implemented but it would be hard to use a platform where we don’t own the dataset (such as dominik’s data).\nGuidelines for how to develop reproducible research through git was seen as a good way, with the idea that the guidance we would produce would (a) provide the target state to aim for and (b) summarize maturity levels (similar to RAP maturity levels) that showcase how researchers can start easily and progress over time."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-1",
    "href": "project-content/meeting-minutes.html#section-1",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Discussion on the proposed project charter for the project (since merged into main)\nUpdate on the 2 repositories for the project (project repo and interim data catalogue repo) and the mocked up project management approach\nRoundtable\n\n\n\n\n\nTeam discussed scope for 2025 CPI EG presentation. The project plan outlined two milestones, one on data catalogue and one on guidance will be the target. The project structure and use of the GitHub project was summarized.\nThe use of the two repos was discussed. The reproducibility-project repo will host the guidance we develop in whatever format we decide (ideas shared including using quarto to write reproducible papers, or a quarto static site if we will more tend towards creating guidance). price-stats-data-catalogue will host the interim open data catalogue.\nScope of the data contract was firmed up - we could aim to catalogue input datasets that are used to create some experimental indices and version the output datasets (that may be price indices or other artifacts) as part of the repository on github (such as by saving them in data folder and formatting them in tidy data format).\nUse of tools like Zenodo was discussed and will be investigated to mint DOIs - ticket #3"
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-2",
    "href": "project-content/meeting-minutes.html#section-2",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Discussion on format for meeting minutes and how to review/approve the minutes each meeting.\nHow to track materials related to reproducibility but are just references to others’ material (not the overall guidance we will provide).\nWhat format is seen as the best way to deliver guidance on reproducibility? It is best to decide an applicable format and stick with it. Options include writing a paper, using a static site, or using the wiki\n\n\n\n\n\nThe team agreed on keeping notes in this meeting-minutes.md file. The team also agreed to the process:\n\nThe note taker would summarize the meeting and would draft a branch and prepare a Pull Request for the team to review the meeting minutes at the next meeting.\nAt the start of the next meeting, the team would review the PR, would commit any changes/fixes needed. and squash and commit the PR into the main branch to approve the minutes.\n\nThe team agreed to track other materials in other-open-materials.md for now\nThe team discussed the means of how guidance will be provided at the end of the day as a lot of material could be included based on the goals in Objective B of our project. Some possible options:\n\na paper (similar to say the FAIR paper on software to be presented at say 2026 Ottawa Group conference) as the main document for guidance and other materials as supporting (like the catalogue). This could use the quarto manuscript format for example.\na static site (again say a quarto one like this one) as the main means of sharing guidance, but also a short paper for the 2026 OG conference as an offline guide\na set of wiki pages similar to other content made by the Task Team\n\nThe team agreed that a static site (the quarto option) is likely the most appropriate as the site can be expanded and maintained as appropriate, a presentation with a link to the site could be provided at conferences.\nThe team discussed planning for the next 1.5 months. The project roadmap outlines the target timelines. As several issues remain unassigned, the team are encouraged to sign up based on bandwidth.\nRoundtable discussion included:\n\nUpdate on Zenodo - which is an option for uploading and citing data (detail in issue 3). There isn’t a process yet identified for datasets that are not owned by the community and where the owner does not upload it to a repository that mints a DOI.\nUpdate on citing data in PriceIndices R package as it has a DOI. Ability to extract the data without installing the package has to be confirmed."
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-3",
    "href": "project-content/meeting-minutes.html#section-3",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Review mock up quarto site for the project, as well as contributing and code of conduct\nReview ongoing work prior to the 2025 CPI Expert Group\nRoundtable\n\n\n\n\n\nTeam discussed the mock up site structure with a home and two main sections, one on how to make your projects reproducible by publishing your code and on using open data.\nThe team also discussed the basic contributing guide and code of conduct.\nThe deliverables to be presented at the 2025 CPI Expert Group were discussed as per the view in the project roadmap.\nAn approach on how to import data from a package was discussed. For instance, how should researchers use data from an R package (such as the PriceIndices package, which has datasets we would like to make list in the interim catalogue) and they wanted to import the data and use it in Python? The team discussed on a phased approach: guidance on how to download the dataset and use it in python from the R ecosystem will be proposed; a longer term approach could be to work with dataset owners to get them to publish it on a repository like Zenodo.\nSupport by the UN Global Platform team for the project was also discussed"
  },
  {
    "objectID": "project-content/meeting-minutes.html#section-4",
    "href": "project-content/meeting-minutes.html#section-4",
    "title": "Meeting minutes of the project team",
    "section": "",
    "text": "Review of skeleton data catalogue\nReview of the changes to the project site, specifically how to cite code and how to cite data sections\nReview roadmap and discussion of topics left to finish in this phase of the project\n\n\n\n\n\nThe team discussed the skeleton of the proof of concecept data catalogue. The technical process to register new datasets is basically to (1) draft a new yaml file in the datasets/ folder using the datacontrac.cli specifications, and then (2) when the PR is accepted (after releveant review) and merged with the main branch, the runner will rerender the catalogue and the dataset will show up.\nThe team discussed next steps. The dataset in #6 is still the third we’d want for presentation at CPI EG.\nThere is a need to differentiate open versus proprietary but popular datasets. Open datasets will be the focus for now with potential for expansion after the conference.\nThe team discussed how to cite datasets and how to cite code topics, and based on the example by the recent Baker et al (2022) FAIR principles for software paper, we decided to go with a nuanced recomemndation for now:\n\nif data or code that a research uses exists should be included in the bibliography\ndata or code that is created as part of the paper should be (ideally published to something that mints a DOI) but the links to the dataset or code are included at the end of the paper under “Data availability” and “Code availability” headers.\nThe idea of topics to discuss after the confernece was also brought up - the process of creating synthetic datasets.\n\nTo support researchers to structure their code, the team also discussed and endoresed recommendign a template RAP."
  }
]