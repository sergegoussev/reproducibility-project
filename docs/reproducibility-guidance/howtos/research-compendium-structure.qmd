---
title: "Research compendium: deeper dive"
subtitle: "Tips on structuring your project for automation, clarity, and reproducibility"
date: 2025-11-25
draft: true
sidebar: true
format:
  html:
    toc: true
    toc-expand: 3
---

The purpose of this guide is on clarifying **why you should care** about structure and automation when it comes to initial setup and use of your research compendium as you are expanding it during your research. While these are slightly more advanced topics (we see [structure as silver level](/docs/reproducibility-guidance/how-to-start.qmd#silver) and [automation as gold](/docs/reproducibility-guidance/how-to-start.qmd#gold)), we think that considering the design at the beginning and a bit of discipline throughout adds great value to yourself and to others! You will indeed thank yourself later!

# The target structure

## Overview of the structure

In a nutshell, along with a resarch paper, a compendium is shared that includes all research objects necessary to reproduce the research. These are often git repositories (in say GitHub) that include a structure similar to the one in @fig-example below.

![Exemplar [price index pipeline](https://github.com/UN-Task-Team-for-Scanner-Data/price-index-pipeline).](/docs/images/price-stats-exemplar.svg){#fig-example fig-align="center" width="35%"}

### A little about each aspect

**A data folder that outlines where to store the raw dataset used for the research project**. Ideally the researcher uses an open dataset (which will make the whole process reproducible), but they may also use a proprietary dataset.

::: callout-tip
## Don't version control the main input dataset

The dataset that acts as the main input dataset to the research should not be version controlled. The folder is created in order to ensure that when a local copy of the compendium is used by researchers, they know where to put the data to ensure that the code will replicate the results in full.

Technically, this means making sure that the `.gitignore` skips this data file
:::

**A folder for functions (or other code) that helps process your data into the relevant outputs.** This could include the code to clean and prepare the raw dataset for research purposes, the code to create the processing and research experiments, as well as notebooks where the data is explored and various aspects that feed the research paper are generated.

**A folder for the output data.** This data can be versioned (if it is not sensitive) with the repository and allows researchers to replicate the process. Note, if the output data can be used for research in its own right, it may be appropriate to register this dataset on a public repository (such as Zenodo) that mints a DOI.

**A folder for documentation to that explain key aspects of the research**. This folder stores [project documentation](https://book.the-turing-way.org/reproducible-research/code-documentation/code-documentation-project) or [the project design](https://book.the-turing-way.org/project-design/pd-overview), however [code should also be documented](https://book.the-turing-way.org/reproducible-research/code-documentation/code-documentation-code) well.

**A license**. This will tell users how they can use the code.

**A `.gitignore` file.** There are some files and folders that should not be version controlled. Notable example is datasets

**A file to [recreate the environment](https://book.the-turing-way.org/reproducible-research/renv) on which the code will run identically**. A shift in one package version to another may change the output, hence its key to track exactly how to replicate the same environment and get the same result.

**Finally, a README to introduce the project**. This will be the landing place when someone navigates to the repository, hence it should describe the project at a glance.

## How to share a research compendium?

Version controlling the research compendium is key.[^5] Once version controlled, it is best to push it to a hosting service like GitHub. Working with something like GitHub makes it easy for researchers to ensure their projects are well structured and robust.

[^5]: See overview and explanations of [version control in The Turing Way](https://book.the-turing-way.org/reproducible-research/vcs) for more info.


# The Why

## Why use a standard directory structure?

As outlined in the ideal case, a structured approach is at a minimum **organized** and **clear**. Anyone can:

1.  Put the same input data as you used in the `/data/` folder.
2.  Run the preprocessing scripts in `/src/processing`.
3.  Repeat the analysis notebooks in `/src/analysis/`and get the same results.

In other words, the research becomes very clear and the *value added* is the contribution, with a lower cognitive load placed on anyone (including yourself at a later date) trying to understand it. The project probably had a messy but implicit structure, you are just making it explicit and coherent.

What happens if you don't make the structure explicit but keep it implicit?

-   When you come to it fresh (either it is new for you or you are coming back to your own material much later), you **waste** time trying to get into the context.

-   The implicit structure (by nature of being implicit) is likely to be messier than if it was well thought through, hence it's more likely to be harder to extend the research,

-   The cognitive burden placed on the prerequisite of understanding something not clearly structured means it's less likely to be material you learn from. In other words, fewer people will review, evaluate or try what you are doing---meaning that your contribution stays in the generalizable category.

## Why take the effort of avoiding manual steps?

Further to structuring your project, avoiding manual steps (especially manipulation of spreadsheets!!) is key. If you've ever read a paper where preprocessing was described in a few short and unclear sentences which made it **nearly impossible** to reproduce the process **without contacting the researcher**---you can probably see what we mean!

So what to do instead?

-   Ensure all steps can be executed through code (and version control this code with the repository). In other words, make sure there are **no manual steps**. This also means that you should version control all code, including the data preprocessing code.

-   Structure the steps to follow a **clear order of operations** and use a specific folder structure for code and data that is processed along the way.

-   **Test the automation** to ensure that anyone (including yourself at a later time) can process the **same outputs** with the **same raw data**.

-   If you're up for it, you could even automate the rendering of the analysis paper, such as through the [manuscripts feature of Quarto](https://quarto-ext.github.io/manuscript-template-jupyter/).

    -   For instance @fig-quarto-to-render shows visually how a `.qmd` file and some notebooks in the repository can be automated to generate the final analysis paper.

    -   Using [make](https://book.the-turing-way.org/reproducible-research/make/) or [dvc](https://github.com/UN-Task-Team-for-Scanner-Data/price-index-pipeline/tree/main) are other examples of tools to automate the process.

    <div>

    ![See [walk-through by Alex Emmons](https://bioinformatics.ccr.cancer.gov/docs/btep-coding-club/CC2024/Quarto/GettingStarted_with_Quarto_orig.html) for more detail.](https://bioinformatics.ccr.cancer.gov/docs/btep-coding-club/CC2024/Quarto/images/quarto_process.png){#fig-quarto-to-render fig-align="center"}

    </div>

## Why invest in documentation?

No matter how clear the structure or the script naming convention, the narrative explanation of steps to follow will still requires some documentation. Indeed the list of specific steps may be clear from the analysis steps, but the **why** and the **how** will not be---hence both will also benefit from being made explicit.

A good way to provide clarity to any user without the context is to create a visual of the process flow. For instance, you can outline the process using [**mermaid diagrams**](https://nhsdigital.github.io/rap-community-of-practice/implementing_RAP/process_mapping/#example-using-markdown-and-mermaid), or use open source diagram tool (like [draw.io](https://app.diagrams.net/) or [lucidchart](https://www.lucidchart.com/pages)) to draw the process yourself, export the image and embed that into the documentation (be sure you version to diagram file itself so that you can always tune the diagram and don't have to recreate it!).[^1]

[^1]: Check out this [overview guide on process mapping by the NHS](https://nhsdigital.github.io/rap-community-of-practice/implementing_RAP/process_mapping/).

::: callout-note
## We use draw.io for reproducibility diagrams

All raw `draw.io` and exports for this site are saved in [`/docs/images/`](https://github.com/UN-Task-Team-for-Scanner-Data/reproducibility-project/tree/main/docs/images)
:::


## Notable example

To showcase an exemplar for price statistics, we created [a mock-up price index pipeline](https://github.com/UN-Task-Team-for-Scanner-Data/price-index-pipeline) that researchers can explore.

